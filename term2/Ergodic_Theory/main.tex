\documentclass[11pt]{article}

%{{{ Packages
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{amsthm}
\usepackage{mathdots}
\usepackage[thicklines]{cancel}
\renewcommand{\CancelColor}{\color{red}}
\usepackage[dvipsnames]{xcolor}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage{microtype}
\usepackage{mathabx}
\usepackage{tikz-cd}
\usepackage{float}
\usepackage{silence}
\WarningFilter{mdframed}{You got a bad break}
\setlength{\parindent}{0pt}
%}}}
%{{{ Custom commands
% Nice maths commands
\newcommand{\defeq}{:=}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|}
%\renewcommand{\dots}{...}
\newcommand{\msrspc}{\ensuremath{(X,\mathcal{B},\mu)}}
\newcommand{\symd}{\triangle}
\newcommand{\indic}[1]{\mathbbm{1}_{#1}}
\newcommand\restr[2]{{% we make the whole thing an ordinary symbol
  \left.\kern-\nulldelimiterspace % automatically resize the bar with \right
  #1 % the function
  \vphantom{\big|} % pretend it's a little taller at normal size
  \right|_{#2} % this is the delimiter
  }}
\newcommand{\relmiddle}[1]{\mathrel{}\middle#1\mathrel{}}
\newcommand{\rmv}{\relmiddle|}
\newcommand{\toitself}{\righttoleftarrow}
\newcommand{\stcmp}{^{\mathsf{c}}}
\newcommand{\mv}[1]{\textbf{#1}}

% Spaces
\newcommand{\ktor}{\mathbb{T}^k}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}

% Ergodic Theory
\newcommand{\gvn}[2]{\ensuremath{\left(#1\;|\;#2\right)}}
\newcommand{\expg}[2]{\ensuremath{\mathbb{E}\gvn{#1}{#2}}}
\newcommand{\infog}[2]{\ensuremath{I\gvn{#1}{#2}}}
\newcommand{\entrg}[2]{\ensuremath{H\gvn{#1}{#2}}}
\newcommand{\probg}[2]{\ensuremath{\mathbb{P}\gvn{#1}{#2}}}
\newcommand{\dm}{\;d\mu}

%}}}
%{{{ Environments
% Definitions environment
\newenvironment{defin}
	{\begin{mdframed}[backgroundcolor=white, roundcorner=5pt, linewidth=1pt]}
	{\end{mdframed}}
\newcommand{\mdf}[1]{{\color{red} #1}}

% Important notes environment
\newenvironment{note}
	{\begin{mdframed}[backgroundcolor=white, linecolor=red, roundcorner=5pt, linewidth=1pt]\bfseries{Note:}\normalfont}
	{\end{mdframed}}

% Examples enviornmnet
\definecolor{mylg}{rgb}{0.9,0.9,0.9}
\newenvironment{eg}
	{\begin{mdframed}[backgroundcolor=mylg,roundcorner=5pt,linewidth=0pt]\bfseries{Example:}\normalfont}
	{\end{mdframed}}

% Theorem enviornment
\newtheorem{prop}{Proposition}[section]
\newtheorem{theorem}[prop]{Theorem}
\newtheorem{lemma}[prop]{Lemma}
\newtheorem{cor}[prop]{Corollary}
%}}}
%{{{ Document metadata
\title{Ergodic Theory Notes}
\author{}
\date{}
%}}}

\begin{document}
\maketitle

\section{Basic Definitions}

For this section we fix a probability space \msrspc and we have a transformation $T:X\to X$ which is measurable in our probability space.

\begin{defin}
	
We say $T$ is a \mdf{measure preserving transformation (m.p.t.)} or $\mu$ is a \mdf{$T$-invariant measure} if 
$$\mu(T^{-1}B)=\mu(B)\quad\forall B\in\mathcal{B}$$

The \mdf{push forward of $\mu$ by $T$} is defiend to be
	$$T_*\mu(B)=\mu(T^{-1}B)\quad\forall B \in\mathcal{B}$$

We say a measure $\mu$ is \mdf{regular} if $\forall B\in\mathcal{B}$ we have $\forall\epsilon >0 \exists U\subseteq X$ open such that
$$B\subseteq U \quad \text{and} \quad \mu(U) < \mu(B) + \epsilon$$

An m.p.t $T$ is said to be \mdf{ergodic} if
$$\forall B\in\mathcal{B},\; T^{-1}B=B \implies \mu(B)=0\;\text{or}\;1$$

\end{defin}

The following theorem will be very useful.

\begin{theorem}[Han-Kolmogorov]
Let $\mathcal{A}$ be an algebra on a space $X$.
Suppose we have a function $\mu_0:\mathcal{A}\to [0, \infty]$ which satisfies
\begin{enumerate}[label=(\roman*)]
	\item \textbf{Finite additivity: }Given $A_1, \dots, A_n\in\mathcal{A}$ disjoint
		\[
			\mu_0\left( \bigcup_{i=1}^n A_i\right)=\sum_{i=1}^{n}\mu_0(A_i)
		\]
	\item \textbf{Sigma additivity: }Given $A_1, A_2, \dots \in \mathcal{A}$ disjoint such that $\bigcup_i A_i\in\mathcal{A}$
		\[
			\mu_0\left( \bigcup_{i=1}^\infty A_i \right) = \sum_{i=1}^{\infty}\mu_0(A_i)
		\]
\end{enumerate}
Then $\mu_0$ extends to a measure on $\sigma(\mathcal{A})$.
Moreover, if $\mu_0$ is $\sigma$-finite then this extension is unique.
\end{theorem}

\section{Facts on Fourier Series}
Suppose $f\in L_1(\ktor)$ then we can define the \mdf{Fourier coefficients} by
$$\hat{f}(n)=\int_{\ktor}f(x)e^{-2\pi in\cdot x}dx\quad\forall n\in\Z^k$$
We define the \mdf{partial Fourier sums} by
\[
	\mdf{S_Nf(x)}\defeq\sum_{\abs{n}\leq N}\hat{f}(n)e^{2\pi i (n\cdot x)}
\]

\begin{theorem}[Riemann-Lebesgue Lemma]
	For all $f\in L_1(\ktor)$, $$\lim_{\abs{n}\to\infty}\hat{f}(n)=0$$
\end{theorem}
\begin{theorem}[Reisz-Fisher Theorem]
$S_nf\to f$ in $L^2$ for all $f\in L^2(\ktor)$.
\end{theorem}
\begin{theorem}[Fej\'er's Theorem]
The average of the partial Fourier sums converges uniformly to $f$, i.e.
$$\frac{1}{N}\sum_{k=0}^{N-1}S_kf\to f\quad\text{uniformly}$$
\end{theorem}
\begin{cor}
If $f\in L^2(\ktor)$ and $\hat{f}(n)=0\;\forall n\in\Z^k\setminus\{0\}$, then $f$ is constant.
\end{cor}
\begin{theorem}
Given $f\in L^2$ which is $T$-invariant
$$\hat{f}(n)=\lim_{N\to\infty}\int (S_N f)(Tx)e^{-2\pi i n\cdot x}$$
\end{theorem}
\section{Criteria for measure preserving}
\begin{theorem}
Given $T:X\to X$ on a probability space $(X,\mu)$, the following are equivalent:
\begin{enumerate}
	\item $T$ is m.p.t
	\item $\int f\circ T d\mu = \int f d\mu \quad\quad \forall f\in L_1(X)$.\end{enumerate}
\end{theorem}
Recall the space $L_1(X)=\left\{ f:x\to\R : \text{measurable}\quad \norm{f}_1 \defeq\int \abs{f} d\mu < \infty \right\}$

\begin{theorem}
Given $T:X\to X$ on a probability space $(X,\mu)$, the following are equivalent:
\begin{enumerate}
	\item $T$ is m.p.t
	\item $\int f\circ T d\mu = \int f d\mu \quad\quad \forall f\in C(X)$.
\end{enumerate}
\end{theorem}

So we see that in fact it suffices to check that $T$ does not affect the integral of any continuous function $f$.
However, we can extend this further using the density of trigonometric polynomials in the space of continuous functions. 
First, we need to define a trigonometric polynomial in arbitrary dimension on the $k$-torus $X=\ktor$ with $\mu=leb$ and $\mathcal{B}=Borel$.
\begin{defin}
	$P:\ktor\to\ktor$ is a \mdf{trigonometric polynomial} if for some $N\geq 1$ and $c_n\in\C$ we can write
	$$P(x)=\sum_{\abs{n}\leq N} c_n e^{2\pi in\cdot x}$$
	where $n=(n_1,\dots,n_k)\in\Z^k, x=(x_1,\dots,x_k), \abs{n}=\abs{n_1}+\dots+\abs{n_k}$.
\end{defin}
\begin{note}
	\[
		\int_{\ktor} e^{2\pi n\cdot x} dx =
		\begin{cases}
			1 & \text{if} \; n=0\\
			0 & \text{if} \; n\neq 0
		\end{cases}
	\]
	and hence
	$$\int_{\ktor}P = c_0$$
\end{note}

\begin{theorem}
Given $T:\ktor\to \ktor$ continuous and denoting by $\mu$ the Lebesgue measure.
\begin{enumerate}
	\item $T$ is m.p.t
	\item $\int P\circ T d\mu = \int P d\mu \quad\quad \forall\; \text{trigonometric polynomials}\; P$. 
\end{enumerate}
\end{theorem}

\section{Criteria for Ergodicity}
First another few definitions.
\begin{defin}
Given $A,B\subseteq X$, their \mdf{symmetric difference} is
$$A\symd B\defeq (A \setminus B)\cup (B \setminus A)$$

A function $f$ is \mdf{$T$-invariant} if $f \circ T = f$ a.e.

A function $f$ is \mdf{constant} if $\exists c\in\R$ such that $f(x) = c$ almost everywhere.
\end{defin}

\begin{theorem}
Given a measure preserving transformation $T:X\to X$ and some $1\leq p\leq\infty$. TFAE:
\begin{enumerate}
	\item T is ergodic.
	\item For all $f$ measurable $f$ invariant $\iff$ $f$ constant.
	\item For all $f\in L^p(X)$, $f$ invariant $\iff$ $f$ constant.
\end{enumerate}
\end{theorem}
\begin{note}
	As a corollary to the Reisz-Fisher theorem, given any $f\in L^2$, if we have that $\hat{f}(n)=0$ for all $n\neq 0$ then $f$ must be constant.
Therefore to check that $T$ is ergodic it suffices to show that all invariant $L^2$ functions have zero Fourier coefficients away from zero.
\end{note}
To this end we present the following formula for computing the Fourier coefficients of invariant $L^2$ functions.
\begin{theorem}
Given $f\in L^2$ which is invariant
$$\hat{f}(n)=\lim_{N\to\infty}\int (S_Nf)(Tx)e^{-2\pi i n\cdot x}dx$$
\end{theorem}

\begin{eg}
	\textbf{The doubling map is ergodic with respect to the Lebesgue Measure.}
\end{eg}

\section{Theorems using Measure Preserving}
\begin{theorem}[Poincar\'e Recurrence Theorem]
	Given a probability space \msrspc and $T:X\to X$ measure preserving. Then
	$$\mu\{x\in B : T^nx\in B \; \text{infinitely often}\,\}=\mu(B)\quad \forall B\in\mathcal{B}$$
\end{theorem}
\section{Theorems using Ergodicity}
\begin{theorem}[Pointwise Ergoic Theorem - Birkhoff 1931]
	Given a measure space \msrspc and a measure preserving transformation $T:X \to X$ and $f\in L^1(X)$. Then $\exists f^*\in L^1(X)$ invariant such that
	$$\frac{1}{n}\sum_{j=0}^{n-1}f\circ T^j \to f^* \; a.e.\quad \text{and} \quad \int f^* = \int f$$
\end{theorem}
Note this does not actually need ergodicity. However, if we additionally assume ergodicity we can prove the following stronger result.
\begin{cor}
Given a probability space \msrspc, $T$ measure preserving and ergodic, $f\in L^1(x)$, then
$$\underbrace{\frac{1}{n}\sum_{j=0}^{n-1}f\circ T^j }_{\text{Time average}}\to \underbrace{\int f d\mu \; a.e.}_{\text{Space average}}$$
\end{cor}
\begin{theorem}[Mean Ergodic Theorems]
	$1\leq p < \infty$, $T$ measure preserving theorem, $f\in L^p(X)$. Define $f^*\defeq \lim_{n\to\infty} \frac{1}{n}\sum_{j=0}^{n-1}f\circ T^j$ almost everywhere. Then
	$$\lim_{n\to\infty}\frac{1}{n}\sum_{j=0}^{n-1}f\circ T^j = f^*$$
	in $L^p$.
\end{theorem}

\begin{proof}
\textbf{Special Case: }$1\leq p < \infty$ but $f\in L^\infty(X)$.

Then by the ergodic theorem and the DCT with dominator $2^p\norm{f}^p$ we have
\[
	\abs{\frac{1}{n}\sum_{j=0}^{n-1}f\circ T^j - f*}^p \to 0
\]

\textbf{General Case: }Take $f\in L^p$

Given $\epsilon >0$ then there is a $g\in L^\infty$ such that $\norm{f-g}_p < \frac{\epsilon}{3}$.
Then we get $f^\ast$ associated to $f$ and $g^\ast$ associated to $g$.
Then $(f-g)^\ast$ is associated to $f-g$ and $(f-g)^\ast=f^\ast - g^\ast$.
By a previous proposition we can see

\[
	\norm{f^\ast-g^\ast}_p = \norm{(f-g)^\ast}_p \leq \norm{f-g}_p < \frac{\epsilon}{3}
\]
Also since $g\in L^\infty$ there must be an $N$ such that
\[
	n\geq N \implies \norm{\frac{1}{n}\sum_{j=0}^{n-1}g\circ T^j - g^\ast}_p < \frac{\epsilon}{3}
\]
Then
\begin{align*}
	\norm{\frac{1}{n}\sum_{j=0}^{n-1}f\circ T^j - f^\ast}_p & \leq \norm{\frac{1}{n}\sum_{j=0}^{n-1}(f-g)\circ T^j}_p +  \underbrace{\norm{\frac{1}{n}\sum_{j=0}^{n-1}g\circ T^j - g^\ast}_p}_{<\epsilon/3 \text{ for }n\geq N} + \underbrace{\norm{g^\ast - f^\ast}}_{<\epsilon/3} \\
															& \leq \frac{1}{n}\sum_{j=0}^{n-1}\norm{(f-g)\circ \cancelto{\text{m.p.t}}{T^j}}_p + \frac{2\epsilon}{3}\\
															& = \norm{f-g} + \frac{2\epsilon}{3} < \epsilon
\end{align*}
\end{proof}

\section{Examples}
\subsection{Linear toral automorphism}
\begin{defin}
A \mdf{linear toral automorphism} is a map $Tx=Ax (mod 1)$ with $A$ a $k\times k$ matrix with integer entries and $\det(A)\neq 0$.

Such an automorphisms is \mdf{hyperbolic} if all eigenvalue for $A$ have $|\lambda|\neq 1$.
\end{defin}
\begin{theorem}
T ergodic $\iff$ no eigenvalue of $A$ is a root of unity.
\end{theorem}
\subsection{Normality of real numbers}
\begin{defin}
$x\in\R$ is \mdf{normal (base b)} if
\begin{itemize}
	\item $x$ has a  unique expansion in that base.
	\item $\forall k\in\{0,1,\dots,b-1\}$
		$$\frac{1}{n}\#\{1\leq i\leq n | x_i=k\}\to\frac{1}{10}\quad\text{as}\;n\to\infty$$
\end{itemize}

$x\in\R$ is \mdf{absolutely normal} if $x$ is normal base b for all $b\geq 2$.
\end{defin}
\begin{theorem}
Almost every $x\in\R$ is absolutely normal.
\end{theorem}

\section{Von Neumann's Ergodic Theorem \& The Adjoint}

\begin{defin}
	Given $T:X\to X$ a measure preserving transformation on a probability space $(X, \mu)$, the \mdf{Koopman operator} is given by
	\[
		\mdf{Uf}\defeq f\circ T
	\]
	for any $f:X\to \R$ measurable.


	Suppose $H$ is a complex Hilbert space with inner product $\langle\cdot, \cdot\rangle$ then a linear operator $U:H\to H$ is an \mdf{isometry} if
	\[
		\norm{Uf}=\norm{f}\quad\forall f\in H
	\]
	where $\norm{f}=\sqrt{\langle f, f\rangle}$.
	Equivalently $\langle Uf, Ug \rangle= \langle f, g\rangle$ for all $f,g \in H$.

	Given a linear operator $U:H\to H$, the \mdf{adjoint} $U*:H\to H$ is the unique bounded linear operator satisfying
	\[
\langle U^\ast f, g \rangle= \langle f, Ug \rangle \quad \forall f, g \in H
	\]

	Let $V\subseteq H$ be a subspace then the \mdf{orthogonal complement} is
	\[
		V^\perp \defeq \left\{ f\in H \rmv \langle f, v \rangle =0 \quad \forall v \in V\right\}
	\]
\end{defin}

\begin{lemma}[Properties of the adjoint]
If $U$ is an isometry then 
\begin{itemize}
	\item $\norm{U^\ast f} \leq \norm{f} \quad \forall f\in H$
	\item $U^\ast U = id $ because
		\[
		\langle U^\ast U f, g \rangle = \langle Uf, Ug\rangle = \langle f, g\rangle \quad \forall f,g \in H
		\]
\end{itemize}
\end{lemma}
\begin{eg}
	\textbf{Computing the adjoint.}
	$X=[0,1]$, $\mu=Leb$, $Tx=2x \mod 1$ and $Uf = f\circ T$ where $U:L^2(X)\toitself$ and our inner product is
	\[
		\langle f, g\rangle \defeq \int_0^1 f \overline{g} \; d\mu
	\]
	\begin{align*}
		\langle U^\ast f, g\rangle & = \langle f, Ug\rangle = \int_0^1 f \overline{Ug} \; dx \\
								   & = \int_0^1 f(x)\overline{g(Tx)}\; dx \\
								   & = \int_0^{\frac{1}{2}} f(x)\overline{g(2x)} \; dx + \int_{\frac{1}{2}}^1 f(x)\overline{g(2x-1)}\; dx \\
								   & = \frac{1}{2}\int_0^1 f\left( \frac{x}{2}\right)\overline{g(x)}\; dx + 
										\frac{1}{2}\int_0^1 f\left( \frac{x+1}{2}\right)\overline{g(x)}\; dx
	\end{align*}
	Hence we can conclude 
	\[
		(U^\ast f)(x) = \frac{1}{2}\left[ f \left( \frac{x}{2}\right) + f \left( \frac{x+1}{2}\right)\right]
	\]
\end{eg}

\begin{prop}
Suppose $U$ is an isometry then
\[
Uf=f \iff U^\ast f = f
\]
\end{prop}

Given a bounded linear operator $A:H\to H$ we can define the \mdf{kernel} to be
\[
	\ker(A) \defeq \left\{ f\in H \rmv Af = 0\right\}
\]
then this a closed subspace in $H$.
Moreover, if $U$ is an isometry then the above proposition tells us that $\ker(U-I)=\ker(U^\ast - I)$.

\textbf{Fact: }
For every closed subspace $V\subseteq H$ we can write $H=V \oplus V^\perp$ and hence
\[
\forall f\in H \quad \exists ! v\in V, w\in V^\perp \; s.t. \; f= v+ w
\]
then we can define \mdf{orthogonal projection} $\pi: H \to V$ by
\[
	\pi(f) = \pi (v+ w) = v
\]

\begin{theorem}[Von Neumann]
If $H$ is a Hilbert space and $U:H\toitself$ is an isometry.
Let $\pi$ denote orthogonal projection into $V=\ker(U-I)$ then
\[
	\frac{1}{n}\sum_{j=0}^{n-1}U^j f \to \pi (f) \quad \text{in }H \quad \text{as} \; n\to \infty
\]
that is
\[
\lim_{n\to\infty}\norm{\frac{1}{n}\sum U^j f - \pi(f)} = 0
\]
\end{theorem}

\begin{proof}
The proof of this is about a page long and definitely warrants a read.
\end{proof}

\begin{cor}
	Given a measure preserving transformation and $Uf=f\circ T$ and $H=L^2(x)$. Then
	\[
		\norm{\frac{1}{n}\sum_{j=0}^{n-1}f\circ T^j - \pi f}_2 \to 0 \quad \text{as} \; n\to\infty
	\]
	If $T$ is ergodic then $\pi f = \int f\; d\mu$.
\end{cor}

\section{Existence of invariant/ergodic measures}
\begin{defin}
Let $\mdf{M(X)}$ be the set of all probability measure on X.

We can view measures as linear functionals on the space of continuous functions as such:
$$\forall f\in C(X)\quad\quad \mdf{\mu(f)}\defeq\int_X f d\mu$$

$\mdf{C(X)^*}\defeq\{\text{bounded linear functionals}\quad w:C(X)\to\R\}$

A linear functional is called \mdf{normalised} if $\int 1 d\mu =1$

A linear functional is called \mdf{positive} if $f\geq 0\implies \int f d\mu \geq 0$
\end{defin}
\begin{theorem}
	Every $\mu\in M(X)$ defines a normalised, positive, bounded, linear functional in $C(X)^*$ defined by $\mu(f)=\int_X f d\mu$.
\end{theorem}
\begin{theorem}[Reisz Representation Theorem]
	Let $w\in C(X)*$ be a bounded linear functional. Suppose that $w$ is positive and normalised. Then $\exists !\mu\in M(X)$ such that $w(f)=\mu(f)$ for all $f\in C(X)$.
\end{theorem}

We would like to give the space $M(X)$ a topology.
Our first idea is the \mdf{strong/norm topology}.
We view $M(X) \subseteq C(X)^\ast$ and inherit the operator norm from $C(X)^\ast$.
That is, given $\mu, \nu \in M(X)$
\[
	d_s(\mu, \nu) \defeq \norm{\mu - \nu} = \sup_{f\in C(X) , \; \norm{f}_\infty=1}\abs{\mu(f)- \lambda(f)} = \sup_{f\in C(X) , \; \norm{f}_\infty=1} \abs{\int f \;d\mu - \int f \; d\lambda}
\]

\begin{note}
	\[
 		\norm{\mu} =1 \quad \forall \mu \in M(X) \subseteq C(X)^\ast
	\]
	since $\abs{\mu(f)}\leq\norm{f}_\infty$ for all $f\in C(X)$ and $\mu(1)=1$.
	Therefore $M(X)$ is a bounded subset of $C(X)^\ast$.
\end{note}

\begin{lemma}
$M(X)$ is closed.
\end{lemma}

\begin{proof}
Suppose we have some sequence $(\mu_n)\subseteq M(X)$ such that $\mu_n\to w\in C(X)^\ast$.
We aim to show that $w=\mu\in M(X)$
We check that the Reisz Representation Theorem is satisfied
\begin{itemize}
	\item Certainly $w\in C(X)^\ast$.
	\item Normalised : $w(1)=\lim_{n\to\infty}\mu_n(1) = \lim_{n\to\infty}1=1$.
	\item Positive: $f\geq 0 \implies \mu_n(f)\geq 0$ for all $n$ and hence $w(f)\geq 0$.
\end{itemize}
\end{proof}

\begin{lemma}
Unfortunately, $M(X)$ is not compact in the strong topology.
\end{lemma}
\begin{proof}
Recall that in a metric space compactness is equivalent to sequential compactness.
So it suffices to find a sequence with no convergent subsequence.
Let $x_1, c_2, \dots \in C$ such that $x_i\neq x_j$ and for all $n$ let $\mu_n=\delta_{x_n}$.

Now take $n\neq m$ we want to show that $\norm{\mu_n - \mu_m} \geq 1$.
For this we define the function 
\[
	f(x)=\frac{d(x, x_n}{d(x, x_n) + d(x, x_m)}
\]
Note since $x_n\neq x_m$ this is well-defined and $f(x_n)=0$ and $f(x_m)=1$.
Moreover, $f$ is continuous and $\norm{f}_\infty = 1$
Therefore $\norm{\delta_{x_n}-\delta_{x_m}}\geq 1$.
\end{proof}

\subsection{Weak $\ast$ topology on $M(X)$}
\begin{defin}
	Let $\mu_n\in M(X)$ and $\mu\in M(X)$.
	We say that $\mu_n\to\mu$ \mdf{weak $\ast$} if
	\[
		\mu_n(f)\to\mu(f) \quad \forall f\in C(X)
	\]
	We can then given $M(X)$ a metric by fixing some countable dense subset $\left\{ f_1, f_2, \dots \right\}\subseteq C(X)$ and defining
	\[
		\mdf{d(\lambda, \mu)}\defeq \sum_{i=1}^{\infty}\frac{1}{2^i}\frac{1}{\norm{f_i}_\infty}\underbrace{\abs{\lambda(f_i)-\mu(f_i)}}_{\leq\norm{f_i}_\infty \int 1 d(\lambda - \mu) \leq \norm{f_i}}\in [0, 1]
	\]
\end{defin}

\begin{prop}
$d$ is a metric.
\end{prop}

\begin{proof}
The difficult thing to prove here is that $\lambda \neq \mu \implies d(\lambda, \mu ) > 0$.
Suppose that we have measures $\lambda \neq \mu$.
By the Reisz Representation Theorem, they must constitute different element of $C(X)^\ast$.
So there is an $f\in C(X)$ such that $\lambda(f)\neq \mu(f)$.
Since the $f_i$ are dense there is some $i$ such that
\[
	\norm{f_i-f}_\infty < \frac{\abs{\lambda(f)-\mu(f)}}{3}
\]
Now
\begin{align*}
	\norm{\lambda(f)-\mu(f)} & \leq \abs{\lambda(f) - \lambda(f_i)} + \abs{\lambda(f_i) - \mu(f_i)} + \abs{\mu(f_i) + \mu(f)} \\
							 & \leq 2 \norm{f_i - f}_\infty + \abs{\lambda(f_i) - \mu(f_i)}\\
							 & < \frac{2\abs{\lambda(f) - \mu(f)}}{3} + \abs{\lambda(f_i) - \mu(f_i)}
\end{align*}
Therefore $\abs{\lambda(f_i)- \mu(f_i)} > \frac{1}{3}\abs{\lambda(f) - \mu(f)} > 0$
So one term of the sum is non-zero and therefore $d(\lambda, \mu) >0$.
\end{proof}

\begin{prop}
$\mu_n \to \mu$ weak $\ast$ $\iff$ $d(\mu_n, \mu)\to 0$ as $n\to\infty$.
\end{prop}

\begin{proof}
Suppose that $\mu_n \to \mu$ weak $\ast$ and choose $\epsilon > 0$.
There exists $M$ such that
\[
	\sum_{i=M}^\infty\frac{1}{2^i}< \frac{\epsilon}{2}
\]
Then 
\[
	d(\mu_n, \mu) \leq \sum_{i=1}^{M}\left[ \frac{1}{2^i}\frac{1}{\norm{f_i}_\infty}\abs{\mu_n(f_i) - \mu(f_i)}\right] + \frac{\epsilon}{2}
\]
Also there is an $N$ such that for $n\geq N$ we can be sure each summand is less than $\frac{\epsilon}{2M}$ since $\mu_n\to \mu$ weak $\ast$ and we only have finitely many $i$ to deal with.
Therefore for any $n\geq N$ we have $d(\mu_n , \mu) \leq \epsilon$.

Conversely, suppose that $d(\mu_n, \mu)\to 0$ then choose $f\in C(X)$ and $\epsilon >0$.
Then there is an $i$ such that $\norm{f_i -f}_\infty < \frac{\epsilon}{3}$.
Also
\[
	\abs{\mu_n(f_i)-\mu(f_i)}\leq 2^i\norm{f_i}d(\mu_n, u) \to 0 \quad \text{as}\; n\to\infty
\]
so there is an $N$ such that for all $n\geq N$ we have $\abs{\mu_n(f_i) - \mu(f_i)} < \frac{\epsilon}{3}$.
Then we can do the normal to trick to show that $\abs{\mu_n - \mu(f)}< \epsilon$.
\end{proof}

\begin{theorem}
$M(X)$ is weak $\ast$ compact.
\end{theorem}

\subsection{Existence of Invariant Measures}
Given $X$ a compact metric space, let $\mathcal{B}$ denote the Borel $\sigma$-algebra and $M(X)$ be defined as before.
Let $T:X\to X$ be a continuous map.
Define
\[
	\mdf{M(X, T)}\defeq \left\{ \mu\in M(X) \rmv T_\ast \mu = \mu\right\}
\]
One can show that for any $f\in C(X)$ we have $T_\ast\mu(f) = \mu(f\circ T)$.
This is proven first for simple functions and then slowly built up.

\begin{theorem}[Krylov-Bogulyvhov]
$M(X, T)\neq \emptyset$.
\end{theorem}

\begin{prop}
$M(X, T)$ is convex.
\end{prop}

\begin{prop}
$M(X, T)$ is weak $\ast$ compact.
\end{prop}

\begin{proof}
Note it suffices to prove that $M(X, T)$ is closed since $M(X, T)\subseteq M(X)$ and $M(X)$ is compact.
In metric spaces we can just make sure all sequences have limits in $M(X, T)$.
Let $\mu_n\in M(X, T)$ such that $\mu_n \to \mu\in M(X)$ weak $\ast$.

Take $f\in C(X)$ then notice
\[
	T_\ast\mu(f) = \mu(f\circ T) \leftarrow \mu_n (f\circ T) = \mu_n (f) \to \mu(f)
\]
By uniqueness of limits and since $f$ was arbitrary we have that $\mu = T_\ast \mu$ and hence $\mu\in M(X, T)$.
\end{proof}

\subsection{Existence of Ergodic Measures}
\begin{defin}
 Let $Y$ be a convex set then $y\in Y$ is called \mdf{extremal} if
 \[
 \exists y_0, y_1 \in Y \quad \text{and} \quad t\in (0, 1)\; s.t. \quad y = (1-t)y_0 + ty_1 \implies y=y_0=y_1
 \]
\end{defin}

\begin{prop}
$\mu\in M(X, T)$ is extremal $\implies$ $\mu$ is ergodic.
\end{prop}

\begin{proof}
Suppose that $\mu$ is not ergodic so there exists $B\in\mathcal{B}$ such that $T^{-1}B = B$ and $\mu(B)\in [0, 1)$.
Then we let
\[
\mu_0(A)\defeq\frac{\mu(A\cap B)}{\mu(B)} \quad \quad \mu_1(A)\defeq\frac{\mu(A\cap B\stcmp)}{\mu(B\stcmp)}
\]
One can show that these define $T$-invariant measures and satisfy
\[
	\mu(B)\mu_0 + \mu(B\stcmp)\mu_1 = \mu
\]
Note that $\mu(B)$ and $\mu(B\stcmp)$ are both in $(0, 1)$ and $\mu_i\neq \mu$ for either $i$.
Hence $\mu$ cannot be extremal.
\end{proof}

For the opposite direction we need the Radon-Nikodym Theorem.

\begin{defin}
	Given measures $\mu, \nu$ on a measure space $(X, \mathcal{B})$	we say that $\nu$ is \mdf{absolutely continuous} with respect to $\nu$ if
	\[
		B\in\mathcal{B}, \quad \mu(B) = 0 \implies \nu(B) = 0
	\]
\end{defin}

\begin{theorem}[Radon-Nikodym Theorem]
Given a measurable space and measures $\mu, \nu$.
Suppose $\mu$ is $\sigma$-finite and $\nu << \mu$.
Then there is a unique $h:X\to [0, +\infty]$ such that
\[
	\forall B\in \mathcal{B} \quad \nu(B) = \int_B h \; d\mu
\]
Then we write $h=\frac{d\nu}{d\mu}$.
\end{theorem}

\begin{prop}
Given $\mu, \nu\in M(X, T)$ and $\nu << \mu$ write $h=\frac{d\nu}{d\mu}$.
Then $h$ is a $T$-invariant function, i.e. $h \circ T = h$.
\end{prop}

\begin{theorem}
$\mu \in M(X, T)$ is ergodic $\implies$ $\mu$ is extremal.
\end{theorem}

\begin{proof}
Suppose that $\mu = (1-t)\mu_0 + t\mu_1$ for some $\mu_0, \mu_1\in M(X, T)$ and $0 < t < 1$.
We aim to show that in fact $\mu=\mu_0 = \mu_1$.
Notice that for any $B\in\mathcal{B}$ we have that $\mu(B) \geq (1-t)\mu_0$ and hence $\mu_0 << \mu$.
The Radon-Nikodym Theorem gives us a unique $h$ such that
\[
	\mu_0(B) = \int_b h \; d\mu
\]
Notice that $\mu_0$ is a probability measure and hence $\int_X h \; d\mu = 1$.
Also we have seen that $h\circ T = h$ and so by ergodicity we must have that $h$ is constant.
But since $\int_X h \; d\mu=1$ we must have that $h\equiv 1 $ almost everywhere and hence $\mu_0 = \mu$.
\end{proof}

\begin{theorem}
Suppose $T: X \to X$ is a continuous map on a compact metric space.
Then there is a $\mu\in M(X, T)$ which is extremal and hence ergodic.
\end{theorem}

\subsection{Abundance and Uniqueness of Ergodic Measures}
\begin{eg}
	Suppose that $T:X \to X$ is continuous and $x_0\in X$ is a fixed point then $\delta_{x_0}\in M(X, T)$ is an ergodic, $T$-invariant measure.

	Suppose that $x_0$ is a periodic points so that $T^qx_0 = x_0$ then define
	\[
		\mu \defeq \frac{1}{q}\sum_{j=0}^{k-1}\delta_{T^jx_0}
	\]
	Then this is also $T$-invariant and ergodic.
	So the doubling map on $S^1$ has a countable infinity of ergodic, invariant probability measures arising in this way.
\end{eg}

So the doubling map has infinitely many ergodic invariant probability measures.
It also has one ergodic absolutely continuous invariant probability measure (a.c.i.p.) which is absolutely continuous wrt $Leb$, namely the Lebesgue measure itself.
Are there any others?

\begin{prop}
Given $T:X\ to X$ a measure preserving transformation and $\mu, \nu \in M(X, T)$
\[
\nu << \mu \implies \nu = \mu
\]
\end{prop}

\begin{proof}
Choose some arbitrary $B\in \mathcal{B}$, we aim to show that $\nu(B)=\mu(B)$.
By the ergodic theorem, there is a set $E_\mu$ such that $\mu(E_\mu)=1$ and
\[
	\forall x \in E_\mu \quad \frac{1}{n}\sum_{j}\chi_b(T^jx)\to \mu(B)
\]
And likewise there is some set $E_\nu$ with $\nu(E_\nu) = 1$ and
\[
	\forall x \in E_\nu \quad \frac{1}{n}\sum_{j}\chi_b(T^jx)\to \nu(B)
\]
If we can find a point in $E_\mu \cap E_\nu$ then by the uniqueness of limits we are done.

Let $h=\frac{d\nu}{d\mu}$.
Then note
\[
	\nu(E_\nu) = \int_{E_\nu}h\; d\mu = \int_X h \; d\mu = 1
\]
because $E_\mu$ has full measure under $\mu$ and also $\nu(X)=1$.
So we see that $\nu(E_\mu)=\mu(E_\nu)=1$ and hence their intersection has full measure and so is non-empty.
\end{proof}

\begin{cor}
	The doubling map has a unique acip (with respect to $Leb$), namely the Lebesgue measure itself.
\end{cor}

\begin{defin}
	We say a measurable $T:X\to X$ is \mdf{uniquely ergodic} if $\abs{M(X, T)}=1$.
\end{defin}

\begin{note}
	If a transformation $T$ is uniquely ergodic then the unique $T$-invariant measure is certainly extremal in $M(X, T)$ and hence must be ergodic, justifying the name.
\end{note}

\begin{prop}
Irrational rotation $T:\mathbb{T}\to \mathbb{T}, \; x\mapsto x_\alpha$, for $\alpha\in\R\setminus\Q$ are uniquely ergodic.
\end{prop}

\begin{proof}
We're in a compact metric space and so we can use the Riesz representation theorem.
We take some arbitrary $\mu\in M(X, T)$ and aim to show that $\int f\;d\mu = \int f \;dLeb$ for all $f\in C(X)$.
We proceed by a density argument using the space of trigonometric polynomials.

Consider $e^{2\pi i n x}$
\[
	\int e^{2\pi i n x} \; d\mu = \int e^{e\pi i n (Tx)} \; d\mu = e^{2\pi i n \alpha}\int e^{2\pi i n x} \; d\mu
\]
Notice $\alpha\not\in \Q$ and hence so long as $n\neq 0$ then we have $e^{2\pi i n \alpha}\neq 1$.
Therefore $\int e^{2\pi i n x} = 0$.

Now choose some arbitrary trig polynomial $P(x) = \sum_{\abs{n}\leq q}c_n e^{e\pi i n x}$
Then we have just shown that $\int P \dm = c_0 = \int P \; dLeb$.
For arbitrary $f\in C(X)$ we proceed by the usual density argument.
\end{proof}

\begin{theorem}
Given a continuous map $T:X\to X$ on a compact metric space, the following are equivalent:
\begin{enumerate}[label=(\alph*)]
	\item For every $f\in C(X)$ there is some $c=c(f)\in \R$ such that
		\[
			\frac{1}{n}\sum_{j}f\circ T^j \to c\quad\text{uniformly on }X
		\]
	\item For every $f\in C(X)$ there is some $c=c(f)\in \R$ such that
		\[
			\frac{1}{n}\sum_{j}f\circ T^j \to c\quad\text{pointwise on }X
		\]
	\item There is some $\mu\in M(X, T)$ such that
		\[
			\frac{1}{n}\sum_{j}f\circ T^j \to \int f\dm \quad\text{pointwise}\quad\forall f\in C(X)
		\]
	\item $T$ is uniquely ergodic.
\end{enumerate}
\end{theorem}

\section{Shifts}
Most of this is verbatim from Dynamical Systems.
There is something on the cylinder generating the Borel $\sigma$-algebra which is probably worth reading.

\subsection{Bernoulli Measures}
Take $\Sigma^+ \defeq \left\{ 1, \dots, k\right\}^\N$ and let $\sigma$ denote the natural shift map.
Fix some $p=(p_1, \dots, p_k)\in \R^q$ such that each $p_i\geq 0$ and $\sum p_i=1$.
Now we define the measure of cylinders by the formula 
\[
	\mu[y_0, \dots, y_m] \defeq p_{y_0}p_{y_1}\dots p_{y_m}
\]
We claim that this extends to a measure on the Borel $\sigma$-algebra.
This will come to be known as a \mdf{Bernoulli measure}.

\begin{prop}
$\mu$ extends uniquely to a probability measure on $\Sigma^+$.
\end{prop}

\begin{proof}
Define $\mathcal{A}\defeq\left\{ \text{finite unions of cylinder sets}\right\}$.
This is an algebra and it generates the Borel $\sigma$-algebra.
Given $A\in\mathcal{A}$ we can write $A$ uniquely as a finite disjoint union of cylinders $C_i$
\[
	A = \bigcup_{i=1}^p C_i \quad \implies \quad \mu(A) = \sum_{i=1}^p \mu(C_i)
\]
We would like to use Hahn-Kolmogorov Theorem and hence we need to show that given $A_1, A_2, \dots \in \mathcal{A}$ which are disjoint such that $\cup_i A_i\in\mathcal{A}$ then $\mu(\cup_i A_i) = \sum_{i}\mu(A_i)$
But in fact any such union must be finite and hence the condition trivial holds.
So we get a unique probability measure on all of $\Sigma^+$, it is certainly a probability measure because
\[
	\mu(\Sigma^+) = \mu(\bigcup_{i=1}^k \left[ i\right] = \sum_{i=1}^k \mu\left[ i\right] =\sum_{i=1}^k p_i = 1
\]
\end{proof}

\begin{prop}
$\mu$ is $\sigma$-invariant.
\end{prop}

\begin{proof}
Note by the uniqueness on $H-K$ it suffices to prove $\mu(\sigma^{-1}A) = \mu(A)$ for each $A\in\mathcal{A}$.
\end{proof}

\begin{prop}
If $A\in\mathcal{A}$ then there is an $n\geq 0$ such that $\mu(A\cap \sigma^{-n}A)=\mu(A)^2$.
\end{prop}

\begin{theorem}
Bernoulli measures are ergodic.
\end{theorem}

\begin{proof}
This is definitely worth proving.
\end{proof}

\begin{cor}
There are uncountably many distinct ergodic, invariant probability measures on $\Sigma^+$.
\end{cor}

\subsection{Markov Measures}
We will need the Perron-Frobenius Theorem.

\begin{theorem}[Perron-Frobenius]
For an aperiodic matrix $B$
\begin{itemize}
	\item $\exists \lambda > 0$ eigenvalue of $B$ such that for all remaining eigenvalues $\lambda' \leq \abs{\lambda}$.
	\item $\lambda$ is simple so there is a unique eigenvector $\mv{v}$ such that $B\mv{v} = \lambda\mv{v}$ and $\sum_{i}v_i=1$.
	\item This $\mv{v}$ is positive, i.e. $v_i > 0$ for every $i$.
	\item If $w$ is another eigenvector then some of its coordinates are positive and others are negative.
\end{itemize}
We call such $\lambda$ the \mdf{maximal eigenvalue}.
\end{theorem}

Consider a subshift of finite type associated to an aperiodic matrix $A\in\left\{ 0, 1\right\}^{k\times k}$.
Choose some $k\times k$ matrix $P$ with non-negative entries such that
\begin{itemize}
	\item $P$ is a \mdf{row-stochastic} if for all $i$, $\sum_{j=1}^k p_{ij} =1$.
	\item $P$ is \mdf{compatible with $A$}, i.e. $p_{ij} >0 \iff A_{ij}=1$.
\end{itemize}

Note that we get the following desirable properties:
\begin{itemize}
	\item $A$ aperiodic $\implies$ $P$ aperiodic since the same choice of $n$ will work.
	\item $(1, 1, \dots, 1)^T$ is an eigenvector with eigenvalue $1$.
	\item By the Perron-Frobenius Theorem, $1$ must be the maximum eigenvalue.
		Therefore, there must be a unique left eigenvector $q$ such that
		\[
			qP = d \quad \sum_{i}q_i = 1\quad q_i >0
		\]
\end{itemize}
We can use this to define a new measure by first defining on cylinder sets
\[
	\mu(\left[ y_0, \dots, y_m \right]) \defeq q_{y_0}P_{y_0 y_1}\dots P_{y_{m-1}y_m}
\]
By the Hahn-Kilogram Theorem this extends to a unique measure on $\Sigma_A^+$ and note
\[
	\mu(\Sigma_A^+)=\mu\left( \bigcup_{i=1}^k [j] \right)=\sum_{j=1}^k \mu( \left[ j\right]) = \sum_{j=1}^k q_j =1
\]
So $\mu$ is a probability measure and $\mu$ is the unique such extension.
We call $\mu$ the \mdf{Markov measure corresponding to $P$}.

\begin{prop}
Markov measures are invariant under the shift map.
\end{prop}

\begin{proof}
\begin{align*}
	\mu(\sigma^{-1}[y_0, \dots, y_m]) &= \mu\left( \cup_{i=1}^k [j, y_0, \dots, y_m]\right) \\
									  &= \sum_{j=1}^k\mu[j, y_0, y_1, \dots, y_m] \\
									  &= \sum_{j=1}^k q_jP_{j y_0} P_{y_0 y_1} \dots P_{y_{m-1} y_m} \\
									  &= \underbrace{\left( \sum_{j=1}^k q_j P_{j y_0}\right)}_{=qy_0}P_{y_0 y_1} \dots P_{y_{m-1} y_m}
									  = \mu[y_0, \dots, y_m]
\end{align*}
\end{proof}

\begin{note}
Markov measure are in fact ergodic but we are not going to prove it.
\end{note}

We can recover Bernoulli measures as a special case of Markov measures.
Given our initial distribution $q=(q_1, \dots, q_k)$ we form the matrix $P$ by
\[
P\defeq
\begin{pmatrix}
	q_1 & \dots & q_k \\
	\vdots & \ddots & \vdots \\
	q_1 & \dots & q_k
\end{pmatrix}
\]
and then we can easily see that $qP=q$ and the corresponding Markov measure coincides with the corresponding Bernoulli measure.

Given any aperiodic matrix $A$ with maximum eigenvalue $\lambda>0$ we know that there are unique vectors $u, v$ such that
\begin{align*}
	Av = \lambda v &\sum_{i}v_i=1 & v_i > 0 \\
	uA^T = \lambda u & \sum_{i}u_i =1 & u_i >0
\end{align*}
Then we define a new matrix $P$ by $P_{ij}\defeq\frac{A_{ij} v_i}{\lambda v_i}$.
We also define a distribution $q_i\defeq\frac{u_i v_i}{c}$ where $c\defeq \sum_{i}u_i v_i$.
Then we can see that $P$ is compatible with $A$ and is now row-stochastic.
Moreover, $qP=q$ and $\sum_{i}q_i = 1$ and $q_i>0$ for all $i$.
The corresponding Markov measure $\mu$ is then called the \mdf{Parry measure}.

\section{Mixing}
\begin{theorem}
Suppose $T:X \to X$ is a measure preserving transformation on $(X,\mathcal{B}, \mu)$.
The following are equivalent:
\begin{enumerate}[label=(\alph*)]
	\item $T$ is ergodic.
	\item For all $A,B\in\mathcal{B}$ we have
		\[
			\frac{1}{n}\sum_{j=0}^{n-1}\mu(T^{-j}A\cap B) \to \mu(A)\mu(B)
		\]
\end{enumerate}
\end{theorem}

\begin{proof}
\textit{(b)}$\implies$\textit{(a)}.
Choose $B\in\mathcal{B}$ such that $T^{-1}B=B$.
Then in the formula take $A=B$ so that $T^{-j}A\cap B=B$.
Then we can see that $\mu(B)=\mu(B)^2$ and hence $\mu(B)\in\left\{ 0, 1\right\}$.

\textit{(a)}$\implies$\textit{(b)}.
We apply the ergodic theorem for $f=\indic{A}$.
We get that
\[
	\frac{1}{n}\sum_{0}^{n-1}\indic{A}\circ T^j \to \mu(A) \quad a.e.
\]
Let's multiply this expression by $\indic{B}$.
\[
	\frac{1}{n}\sum_{j=0}^{n-1}\indic{T^{-j}A\cap B} = \frac{1}{n}\sum_{j=0}^{n-1}(\indic{A}\circ T^j)\indic{B} \to \mu(A)\indic{B}\quad a.e.
\]
Then we integrated both sides and use the dominated convergence theorem since the left hand side is bounded by $1$ which has finite integral.
Hence
\[
	\frac{1}{n}\sum_{j=0}^{n-1}\mu(T^{-j}A\cap B) = \int \frac{1}{n}\sum_{j=0}^{n-1}\indic{T^{-j}A\cap B} \to \int \mu(A) \indic{B} = \mu(A) \mu(B)
\]
\end{proof}

\begin{defin}
	We say that $T$ is \mdf{weak mixing} if
	\[
		\frac{1}{n}\sum_{j=0}^{n-1}\abs{\mu(T^{-j}A\cap B) - \mu(A)\mu(B)} \to 0
	\]
	and say that $T$ is \mdf{(strong) mixing} if
	\[
		\mu(T^{-n}A\cap B)) \to \mu(A)\mu(B)
	\]
	each for all $A, B\in\mathcal{B}$.

	A subset $J\subseteq \N$ is of \mdf{density $d$} if
	\[
		\frac{\abs{J\cap \left\{ 0, 1, \dots , n-1\right\}}}{n}\to d\quad \text{as} \; n\to\infty
	\]
	We say $J$ has
	\begin{itemize}
		\item \mdf{full density} if $d=1$,
		\item \mdf{zero density} if $d=0$, and
		\item \mdf{positive density} if $d>0$.
	\end{itemize}
\end{defin}

Notes on density:
\begin{itemize}
	\item If $d$ exists then certainly $d\in \left[ 0, 1\right]$.
	\item If $J$ has density $d$ then $\N \setminus J$ has density $1-d$.
	\item If $T$ is ergodic and $B\in\mathcal{B}$ is a Borel set then
		\[
			J\defeq\left\{ n \geq 0 \rmv T^n x \in B\right\}\text{ has density }\mu(B) \text{ for a.e. }x
		\]
\end{itemize}

\begin{proof}
\[
	\frac{\abs{J\cap \left\{ 0, 1, \dots, n-1\right\}}}{n}= \frac{1}{n}\sum_{j=0}^{n-1}\indic{B}(T^j x) \to \mu(B)
\]
by the ergodic theorem.
\end{proof}

\begin{lemma}
Let $a_n\in\R$ be a bounded sequence.
The following are equivalent:
\begin{enumerate}
	\item $\lim_{n\to\infty}\frac{1}{n}\sum_{j=0}^{n-1}\abs{a_j} = 0$
	\item There is a subset $J\subseteq\N$ of full density such that $\lim_{n\to\infty, \; n\in J}a_n =0$.
\end{enumerate}
\end{lemma}

\begin{proof}
The proof of this is unreasonably long.
\end{proof}

\begin{cor}
	T is weak-mixing if and only if for all $A,B \in \mathcal{B}$ there is some $J\subseteq\N$ of full density such that
	\[
		\lim_{n\to\infty, \; n\in J}\mu(A\cap T^{-n}B) = \mu(A)\mu(B)
	\]
\end{cor}

\begin{proof}
Take $a_n = \mu(A \cap T^{-n}B) - \mu(A)\mu(B)$ in the above lemma.
\end{proof}

\begin{theorem}
$T$ is weak mixing $\implies$ $T\times T$ is ergodic (in fact weak mixing).
Note $T\times T$ is taking place under the product measure on the product $\sigma$-algebra.
\end{theorem}

\begin{cor}
Rotations of the circle are not weak mixing.
\end{cor}

\section{Entropy}
The motivation for a definition of entropy is as a vehicle to distinguish between dynamical systems. First we need to know how tell when two systems are identical.
\begin{defin}
	Two probability spaces with measure preserving transformations, $(X,\mathcal{B},\mu,T),(Y,\mathcal{C}, \nu, S)$ are \mdf{measure-theoretically isomorphic} if there exists a bijection $\pi:B\to C$ where $B\in\mathcal{B}$ and $C\in\mathcal{C}$ such that
	\begin{itemize}
		\item $\mu(B)=\nu(C)=1$
		\item $T(B)\subseteq B, S(C)\subseteq C$
		\item $\pi: B\to C$ and $\pi^{-1}:C\to B$ are measure preserving transformations
		\item $\pi\circ T = S\circ\pi$
		\begin{figure}[H]
			\centering
			\begin{tikzcd}
				X \arrow[r, "T"] \arrow[d, "\pi"'] & X \arrow[d, "\pi"] \\
				Y \arrow[r, "S"'] & Y
			\end{tikzcd}
		\end{figure}	
	\end{itemize}
	
Assume $\msrspc$ is a probability space and $\alpha=\{A_i\}$ a countable collection of subsets $A_i\subseteq B$.
\begin{itemize}
	\item We say $\alpha$ is a \mdf{partition} of $X$ if $\cup A_i$ = X and $A_i\cap A_j=\emptyset$ up to measure 0.
	\item The \mdf{join} of two partitions $\alpha,\beta$ is the partition $\alpha\vee\beta$ of all possible intersections $A_i\cap B_j$.
	\item A countable partition $\beta$ is a \mdf{refinement} of $\alpha$ if every element of $\alpha$ is a union of element of $\beta$ and write $\alpha\leq\beta$.
	\item $\alpha,\beta$ are \mdf{independent} if $\mu(A\cap B)=\mu(A)\mu(B)$ for all $A\in\alpha, B\in\beta$.
	\item The \mdf{information of a partition} $\alpha$ is
		$$I(\alpha)\defeq-\sum_{A\in\alpha}\indic{A}\log(\mu(A))$$
		where $I(\alpha):X\to[0,\infty]$.
	\item The \mdf{entropy of a partition} $\alpha$ is
		$$H(\alpha)\defeq\int_X I(\alpha)d\mu=-\sum_{A\in\alpha}\mu(A)\log(\mu(A))$$
		using the convention $0\cdot\log(0)=0$.
	\item The \mdf{expectation given a partition} is
		$$\expg{\cdot}{\alpha}\defeq\expg{\cdot}{\sigma(\alpha)}$$
	\item The \mdf{conditional probability} of $B\in\mathcal{B}$ given $\alpha$ is
		$$\probg{B}{\alpha}\defeq\expg{\indic{B}}{\alpha}$$
\end{itemize}
Suppose that $\mathcal{C}$ is a sub $\sigma$-algebra of $\mathcal{B}$.
\begin{itemize}
	\item The \mdf{conditional information of $\alpha$ given $\mathcal{C}$} is
		$$\infog{\alpha}{\mathcal{C}}\defeq - \sum_{A\in\alpha}\indic{A}\log(\mu\gvn{A}{C})$$
	where $\mu\gvn{A}{\mathcal{C}}\defeq\expg{\indic{A}}{\mathcal{C}}$
	\item The \mdf{conditional entropy of $\alpha$ given $\mathcal{C}$} is
			$$\entrg{\alpha}{\mathcal{C}}\defeq\int_X\infog{\alpha}{\mathcal{C}}d\mu$$	
\end{itemize}
\end{defin}
We have the following desirable properties:
\begin{itemize}
	\item If $\alpha$ and $\beta$ are independent then $I(\alpha\vee\beta)=I(\alpha)+I(\beta)$.
	\item If $\alpha=\{X\}$ then $I(\alpha)=0$ so $H(\alpha)=0$.
	\item If $T$ is a measure preserving transformation then $H(T^{-1}\alpha)=H(\alpha)$.
	\item Given $A\in\alpha$, $\restr{\expg{f}{\alpha}}{A}=\frac{\int_A f \;d\mu}{\mu(A)}$ and hence
		$$\expg{f}{\alpha}=\sum_{A\in\alpha}\indic{A}\frac{\int_A f \;d\mu}{\mu(A)}$$
	\item Conditional probability and expectation are constant on partition elements.
	\item For $A\in\alpha$, 
		$$\restr{\probg{B}{\alpha}}{A}=\restr{\expg{\indic{B}}{\alpha}}{A}=\frac{\int_A \indic{B} d\mu}{\mu(A)}=\frac{\mu(A\cap B}{\mu(A)}$$
	\item If $\mathcal{C}=\{X,\emptyset\}$ then $\infog{\alpha}{\mathcal{C}}=I(\alpha)$ and $\entrg{\alpha}{\mathcal{C}}=H(\alpha)$.
	\item If $g\geq 0$ is $\sigma(\alpha)$-measurable then $\expg{fg}{\sigma(\alpha)}=g\cdot\expg{f}{\sigma(\alpha)}$.
	\item If $T$ is a measure preserving transformation then $\infog{T^{-1}\alpha}{T^{-1}\mathcal{C}}=\infog{\alpha}{\mathcal{C}}\circ T$.
	\item Integrating this gives $\entrg{T^{-1}\alpha}{T^{-1}\mathcal{C}}=\entrg{\alpha}{\mathcal{C}}$.
	\item $\alpha\leq\beta\implies\infog{\alpha}{\beta}=0$.
\end{itemize}


\begin{prop}
	$$\entrg{\alpha}{\mathcal{C}}=-\int_X \sum_{A\in\alpha}\mu\gvn{A}{\mathcal{C}}\log(\mu\gvn{A}{\mathcal{C}})d\mu$$	
\end{prop}
\begin{lemma}[Basic Identity]
	Given $\alpha,\beta,\gamma$ partitions of $X$ then
	$$\infog{\alpha\vee\beta}{\gamma}=\infog{\alpha}{\gamma}+\infog{\beta}{\alpha\vee\gamma}$$
	$$\entrg{\alpha\vee\beta}{\gamma}=\entrg{\alpha}{\gamma}+\entrg{\beta}{\alpha\vee\gamma}$$
\end{lemma}
\begin{cor}
	$$\beta\leq\gamma\implies\infog{\alpha\vee\beta}{\gamma}=\infog{\alpha}{\gamma}$$
\end{cor}
\begin{cor}[Monotonicity of information of entropy]
	$$\alpha\leq\beta\implies\infog{\alpha}{\gamma}\leq\infog{\beta}{\gamma}$$
\end{cor}
\begin{cor}[Anti-monotonicity of entropy]
	$$\beta\leq\gamma\implies\entrg{\alpha}{\beta}\geq\entrg{\alpha}{\gamma}$$
\end{cor}
\begin{cor}
We have the two following properties as well:
\begin{itemize}
	\item $\entrg{\alpha}{\gamma}\leq H(\alpha)$ (because always $\gamma\leq\{X,\emptyset\}$)
	\item $\entrg{\alpha\vee\beta}{\gamma}\leq\entrg{\alpha}{\gamma}+\entrg{\beta}{\gamma}$
\end{itemize}
\end{cor}
So far this does not encapsulate any dynamics of the system and so we must use these concepts to arrive at a definition of entropy which depends on the transformation. For convenience define the following set:
$$\mathcal{P}\defeq\{\alpha\;\text{countable partitions}\;|H(\alpha) < \infty\}$$
Now choose $\alpha\in\mathcal{P}$. Then we define the following:
$$\mdf{H_n(\alpha)}\defeq H(\alpha^n)\quad\text{where}\quad\alpha^n\defeq\bigvee_{j=0}^{n-1}T^{-j}\alpha$$
This has the convenient property that $H_{n+m}(\alpha)\leq H_n(\alpha)+H_m(\alpha)$, i.e. these $H_n$ form a sub-additive sequence $\R$-valued sequence and hence the limit $\mdf{h(T,\alpha)}\defeq\lim_{n\to\infty}\frac{1}{n}H_n(\alpha)$ exists. We call this the \mdf{entropy of T relative to $\alpha$}.
We can then define the \mdf{entropy of T} by taking the supremum:
$$\mdf{h(T)}\defeq\sup_{\alpha\in\mathcal{P}}h(T,\alpha)$$
Having done all this work, this had better be a measure-theoretic isomorphism invariant.

\end{document}
