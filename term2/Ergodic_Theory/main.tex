\documentclass[11pt]{article}

%{{{ Packages
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{amsthm}
\usepackage{mathdots}
\usepackage[thicklines]{cancel}
\renewcommand{\CancelColor}{\color{Red}}
\usepackage[dvipsnames]{xcolor}
\usepackage{witharrows}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage{bm}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{tikz-cd}
\usepackage{float}
\usepackage{silence}
\WarningFilter{mdframed}{You got a bad break}

\setlength{\parindent}{0pt}
%}}}
%{{{ Custom commands
% Nice maths commands
\newcommand{\defeq}{:=}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|}
%\renewcommand{\dots}{...}
\newcommand{\msrspc}{\ensuremath{(X,\mathcal{B},\mu)}}
\newcommand{\symd}{\triangle}
\newcommand{\indic}[1]{\mathbbm{1}_{#1}}
\newcommand\restr[2]{{% we make the whole thing an ordinary symbol
  \left.\kern-\nulldelimiterspace % automatically resize the bar with \right
  #1 % the function
  \vphantom{\big|} % pretend it's a little taller at normal size
  \right|_{#2} % this is the delimiter
  }}
\newcommand{\relmiddle}[1]{\mathrel{}\middle#1\mathrel{}}
\newcommand{\rmv}{\relmiddle|}
\newcommand{\stcmp}{^{\mathsf{c}}}
\newcommand{\mv}[1]{\textbf{#1}}

% Spaces
\newcommand{\ktor}{\mathbb{T}^k}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}

% Ergodic Theory
\DeclareMathOperator{\Cov}{\mathrm{Cov}}
\DeclareMathOperator{\lip}{\mathrm{lip}}
\DeclareMathOperator{\diam}{\mathrm{diam}}
\newcommand{\gvn}[2]{\ensuremath{\left(#1\rmv#2\right)}}
\newcommand{\expg}[2]{\ensuremath{\mathbb{E}\gvn{#1}{#2}}}
\newcommand{\infog}[2]{\ensuremath{I\gvn{#1}{#2}}}
\newcommand{\entrg}[2]{\ensuremath{H\gvn{#1}{#2}}}
\newcommand{\probg}[2]{\ensuremath{\mathbb{P}\gvn{#1}{#2}}}
\newcommand{\dm}{\;d\mu}
\newcommand{\toitself}{\mathbin{\scalebox{.85}{%
    \lefteqn{\scalebox{.5}{$\blacktriangleleft$}}\raisebox{.34ex}{$\supset$}}}}

% Comutative diagrams
% From https://tex.stackexchange.com/questions/409286/strike-out-arrow-in-commutative-diagram-using-tikzcd
\usetikzlibrary{decorations.markings}
\tikzset{
  negate/.style={
    decoration={
      markings,
      mark= at position 0.5 with {
        \node[transform shape] (tempnode) {$/$};
      },
    },
    postaction={decorate},
  },
}

%}}}
%{{{ Environments
% Definitions environment
\newenvironment{defin}
	{\begin{mdframed}[backgroundcolor=white, roundcorner=5pt, linewidth=1pt, linecolor=RoyalBlue]}
	{\end{mdframed}}
	\newcommand{\mdf}[1]{{\color{RoyalBlue} #1}}

% Important notes environment
\newenvironment{note}
	{\begin{mdframed}[backgroundcolor=white, linecolor=RubineRed, roundcorner=5pt, linewidth=1pt]\bfseries{Note:}\normalfont}
	{\end{mdframed}}

% Examples enviornmnet
\definecolor{mylg}{rgb}{0.9,0.9,0.9}
\newenvironment{eg}
	{\begin{mdframed}[backgroundcolor=mylg,roundcorner=5pt,linewidth=0pt]\bfseries{Example:}\normalfont}
	{\end{mdframed}}

% Highlighting
	\newcommand{\highlight}[1]{{\color{RubineRed} #1}}

% Theorem enviornment
\newtheorem{prop}{Proposition}[section]
\newtheorem{theorem}[prop]{Theorem}
\newtheorem{lemma}[prop]{Lemma}
\newtheorem{cor}[prop]{Corollary}
%}}}
%{{{ Document metadata
\title{Ergodic Theory Notes}
\author{Thomas Chaplin}
\date{}
%}}}

\begin{document}
\maketitle
\tableofcontents
\newpage

\section{Basic Definitions}

For this section we fix a probability space \msrspc and we have a transformation $T:X\to X$ which is measurable in our probability space.

\begin{defin}
	
We say $T$ is a \mdf{measure preserving transformation (m.p.t.)} or $\mu$ is a \mdf{$T$-invariant measure} if 
$$\mu(T^{-1}B)=\mu(B)\quad\forall B\in\mathcal{B}$$

The \mdf{push forward of $\mu$ by $T$} is defiend to be
	$$T_*\mu(B)=\mu(T^{-1}B)\quad\forall B \in\mathcal{B}$$

We say a measure $\mu$ is \mdf{regular} if $\forall B\in\mathcal{B}$ we have $\forall\epsilon >0 \exists U\subseteq X$ open such that
$$B\subseteq U \quad \text{and} \quad \mu(U) < \mu(B) + \epsilon$$

An m.p.t $T$ is said to be \mdf{ergodic} if
$$\forall B\in\mathcal{B},\; T^{-1}B=B \implies \mu(B)=0\;\text{or}\;1$$

\end{defin}

The following theorem will be very useful.

\begin{theorem}[Hahn-Kolmogorov]
Let $\mathcal{A}$ be an algebra on a space $X$.
Suppose we have a function $\mu_0:\mathcal{A}\to [0, \infty]$ which satisfies
\begin{enumerate}[label=(\roman*)]
	\item \textbf{Finite additivity: }Given $A_1, \dots, A_n\in\mathcal{A}$ disjoint
		\[
			\mu_0\left( \bigcup_{i=1}^n A_i\right)=\sum_{i=1}^{n}\mu_0(A_i)
		\]
	\item \textbf{Sigma additivity: }Given $A_1, A_2, \dots \in \mathcal{A}$ disjoint such that $\bigcup_i A_i\in\mathcal{A}$
		\[
			\mu_0\left( \bigcup_{i=1}^\infty A_i \right) = \sum_{i=1}^{\infty}\mu_0(A_i)
		\]
\end{enumerate}
Then $\mu_0$ extends to a measure on $\sigma(\mathcal{A})$.
Moreover, if $\mu_0$ is $\sigma$-finite then this extension is unique.
\end{theorem}

\section{Facts on Fourier Series}
Suppose $f\in L_1(\ktor)$ then we can define the \mdf{Fourier coefficients} by
$$\hat{f}(n)=\int_{\ktor}f(x)e^{-2\pi in\cdot x}dx\quad\forall n\in\Z^k$$
We define the \mdf{partial Fourier sums} by
\[
	\mdf{S_Nf(x)}\defeq\sum_{\abs{n}\leq N}\hat{f}(n)e^{2\pi i (n\cdot x)}
\]

\begin{theorem}[Riemann-Lebesgue Lemma]
	For all $f\in L_1(\ktor)$, $$\lim_{\abs{n}\to\infty}\hat{f}(n)=0$$
\end{theorem}
\begin{theorem}[Riesz-Fisher Theorem]
$S_nf\to f$ in $L^2$ for all $f\in L^2(\ktor)$.
\end{theorem}
\begin{theorem}[Fej\'er's Theorem]
The average of the partial Fourier sums converges uniformly to $f$, i.e.
$$\frac{1}{N}\sum_{k=0}^{N-1}S_kf\to f\quad\text{uniformly}$$
\end{theorem}
\begin{cor}
If $f\in L^2(\ktor)$ and $\hat{f}(n)=0\;\forall n\in\Z^k\setminus\{0\}$, then $f$ is constant.
\end{cor}
\begin{theorem}
Given $f\in L^2$ which is $T$-invariant
$$\hat{f}(n)=\lim_{N\to\infty}\int (S_N f)(Tx)e^{-2\pi i n\cdot x}$$
\end{theorem}
\section{Criteria for Measure Preserving}
\begin{theorem}
Given $T:X\to X$ on a probability space $(X,\mu)$, the following are equivalent:
\begin{enumerate}
	\item $T$ is m.p.t
	\item $\int f\circ T d\mu = \int f d\mu \quad\quad \forall f\in L_1(X)$.\end{enumerate}
\end{theorem}
Recall the space $L_1(X)=\left\{ f:x\to\R : \text{measurable}\quad \norm{f}_1 \defeq\int \abs{f} d\mu < \infty \right\}$

\begin{theorem}
Given $T:X\to X$ on a probability space $(X,\mu)$, the following are equivalent:
\begin{enumerate}
	\item $T$ is m.p.t
	\item $\int f\circ T d\mu = \int f d\mu \quad\quad \forall f\in C(X)$.
\end{enumerate}
\end{theorem}

So we see that in fact it suffices to check that $T$ does not affect the integral of any continuous function $f$.
However, we can extend this further using the density of trigonometric polynomials in the space of continuous functions. 
First, we need to define a trigonometric polynomial in arbitrary dimension on the $k$-torus $X=\ktor$ with $\mu=leb$ and $\mathcal{B}=Borel$.
\begin{defin}
	$P:\ktor\to\ktor$ is a \mdf{trigonometric polynomial} if for some $N\geq 1$ and $c_n\in\C$ we can write
	$$P(x)=\sum_{\abs{n}\leq N} c_n e^{2\pi in\cdot x}$$
	where $n=(n_1,\dots,n_k)\in\Z^k, x=(x_1,\dots,x_k), \abs{n}=\abs{n_1}+\dots+\abs{n_k}$.
\end{defin}
\begin{note}
	\[
		\int_{\ktor} e^{2\pi n\cdot x} dx =
		\begin{cases}
			1 & \text{if} \; n=0\\
			0 & \text{if} \; n\neq 0
		\end{cases}
	\]
	and hence
	$$\int_{\ktor}P = c_0$$
\end{note}

\begin{theorem}
Given $T:\ktor\to \ktor$ continuous and denoting by $\mu$ the Lebesgue measure.
\begin{enumerate}
	\item $T$ is m.p.t
	\item $\int P\circ T d\mu = \int P d\mu \quad\quad \forall\; \text{trigonometric polynomials}\; P$. 
\end{enumerate}
\end{theorem}

\section{Criteria for Ergodicity}
First another few definitions.
\begin{defin}
Given $A,B\subseteq X$, their \mdf{symmetric difference} is
$$A\symd B\defeq (A \setminus B)\cup (B \setminus A)$$

A function $f$ is \mdf{$T$-invariant} if $f \circ T = f$ a.e.

A function $f$ is \mdf{constant} if $\exists c\in\R$ such that $f(x) = c$ almost everywhere.
\end{defin}

\begin{cor}
The following are equivalent:
\begin{enumerate}
	\item $T$ is ergodic.
	\item $B\in \mathcal{B}$ and $\mu(T^{-1}(B) \symd B) = 0$ $\implies$ $\mu(B) = 0 $ or $1$.
\end{enumerate}
\end{cor}

\begin{theorem}
Given a measure preserving transformation $T:X\to X$ and some $1\leq p\leq\infty$. TFAE:
\begin{enumerate}
	\item T is ergodic.
	\item For all $f$ measurable $f$ invariant $\iff$ $f$ constant.
	\item For all $f\in L^p(X)$, $f$ invariant $\iff$ $f$ constant.
\end{enumerate}
\end{theorem}

\begin{proof}
The difficult one to show is \textit{(1)}$\implies$\textit{(2)}.
Suppose that $f$ is measurable and $f\circ T = f$ almost everywhere but $f$ is not constant.
So there is some $y\in \R$ such that $\mu\left\{ f > y\right\}>0$ and $\mu\left\{ f < y\right\}>0$.
Then $T^{-1}\left\{ f > y \right\}\symd \left\{ f > y\right\} = \left\{ f \circ T > y\right\} \symd \left\{ f > y \right\} \subseteq \left\{ f\circ T \neq f\right\}$.
Hence $\mu(T^{-1}\left\{ f > y\right\} \symd \left\{ f>y\right\}) \leq \mu(\left\{ f \circ T \neq f\right\}) = 0$.
So by the corollary, $\mu(\left\{ f > y\right\}) = 0$ or $1$.
Either case leads to a contradiction.
\end{proof}

\begin{note}
	As a corollary to the Riesz-Fisher theorem, given any $f\in L^2$, if we have that $\hat{f}(n)=0$ for all $n\neq 0$ then $f$ must be constant.
Therefore to check that $T$ is ergodic it suffices to show that all invariant $L^2$ functions have zero Fourier coefficients away from zero.
\end{note}
To this end we present the following formula for computing the Fourier coefficients of invariant $L^2$ functions.
\begin{theorem}
\label{th:coeffcompute}
Given $f\in L^2$ which is invariant
$$\hat{f}(n)=\lim_{N\to\infty}\int (S_Nf)(Tx)e^{-2\pi i n\cdot x}dx$$
\end{theorem}

\begin{eg}
	\textbf{Recipe for ergodicity:}

	We're going to show that the any invariant function must be constant by showing that all Fourier coefficients (away from $n=0$) are $0$.

	\begin{enumerate}
		\item Compute the partial sum $S_Nf(Tx)$.
		\item Use Theorem \ref{th:coeffcompute} to obtain a relationship between Fourier coefficients.
		\item Use this relationship to equate absolute values of a sequences of Fourier coefficients.
		\item Use the Riemann-Lebesgue Lemma to conclude that all coefficients away from $n=0$ are vanishing.
	\end{enumerate}
\end{eg}

\begin{eg}
	\textbf{The doubling map is ergodic with respect to the Lebesgue Measure.}
\end{eg}

\section{Theorems using Measure Preserving}
\begin{theorem}[Poincar\'e Recurrence Theorem]
	Given a probability space \msrspc and $T:X\to X$ measure preserving. Then
	$$E\defeq\mu\{x\in B : T^nx\in B \; \text{infinitely often}\,\}=\mu(B)\quad \forall B\in\mathcal{B}$$
\end{theorem}

\begin{proof}
\begin{enumerate}
	\item Define $F$ to be the set of points in $B$ who never return.
		\[
			F\defeq \left\{ x\in B \rmv T^nx\not\in B \quad \forall n\geq 1\right\}
		\]
		Then note the $T^{-n}F$ are all disjoint and of the same measure so we must have $\mu(F) = 0$.
	\item Notice
		\[
			x\not \in E \iff \exists k \geq 1 \quad s.t. \quad T^kx \in F
		\]
		and hence $B\setminus E = \cup_k T^{-k}F$ and so $\mu(B \setminus E) = 0$.
		We can conclude $\mu(E) = \mu(B)$.
\end{enumerate}
\end{proof}

\section{Theorems using Ergodicity}
\begin{theorem}[Pointwise Ergoic Theorem - Birkhoff 1931]
	Given a measure space \msrspc and a measure preserving transformation $T:X \to X$ and $f\in L^1(X)$. Then $\exists f^*\in L^1(X)$ invariant such that
	$$\frac{1}{n}\sum_{j=0}^{n-1}f\circ T^j \to f^* \; a.e.\quad \text{and} \quad \int f^* = \int f$$
\end{theorem}
Note this does not actually need ergodicity. However, if we additionally assume ergodicity we can prove the following stronger result.
\begin{cor}
Given a probability space \msrspc, $T$ measure preserving and ergodic, $f\in L^1(x)$, then
$$\underbrace{\frac{1}{n}\sum_{j=0}^{n-1}f\circ T^j }_{\text{Time average}}\to \underbrace{\int f d\mu \; a.e.}_{\text{Space average}}$$
\end{cor}
\begin{theorem}[Mean Ergodic Theorems]
	$1\leq p < \infty$, $T$ measure preserving theorem, $f\in L^p(X)$. Define $f^*\defeq \lim_{n\to\infty} \frac{1}{n}\sum_{j=0}^{n-1}f\circ T^j$ almost everywhere. Then
	$$\lim_{n\to\infty}\frac{1}{n}\sum_{j=0}^{n-1}f\circ T^j = f^*$$
	in $L^p$.
\end{theorem}

\begin{proof}
\textbf{Special Case: }$1\leq p < \infty$ but $f\in L^\infty(X)$.

Then by the ergodic theorem and the DCT with dominator $2^p\norm{f}^p$ we have
\[
	\abs{\frac{1}{n}\sum_{j=0}^{n-1}f\circ T^j - f*}^p \to 0
\]

\textbf{General Case: }Take $f\in L^p$

Given $\epsilon >0$ then there is a $g\in L^\infty$ such that $\norm{f-g}_p < \frac{\epsilon}{3}$.
Then we get $f^\ast$ associated to $f$ and $g^\ast$ associated to $g$.
Then $(f-g)^\ast$ is associated to $f-g$ and $(f-g)^\ast=f^\ast - g^\ast$.
By a previous proposition we can see

\[
	\norm{f^\ast-g^\ast}_p = \norm{(f-g)^\ast}_p \leq \norm{f-g}_p < \frac{\epsilon}{3}
\]
Also since $g\in L^\infty$ there must be an $N$ such that
\[
	n\geq N \implies \norm{\frac{1}{n}\sum_{j=0}^{n-1}g\circ T^j - g^\ast}_p < \frac{\epsilon}{3}
\]
Then
\begin{align*}
	\norm{\frac{1}{n}\sum_{j=0}^{n-1}f\circ T^j - f^\ast}_p & \leq \norm{\frac{1}{n}\sum_{j=0}^{n-1}(f-g)\circ T^j}_p +  \underbrace{\norm{\frac{1}{n}\sum_{j=0}^{n-1}g\circ T^j - g^\ast}_p}_{<\epsilon/3 \text{ for }n\geq N} + \underbrace{\norm{g^\ast - f^\ast}}_{<\epsilon/3} \\
															& \leq \frac{1}{n}\sum_{j=0}^{n-1}\norm{(f-g)\circ \cancelto{\text{m.p.t}}{T^j}}_p + \frac{2\epsilon}{3}\\
															& = \norm{f-g} + \frac{2\epsilon}{3} < \epsilon
\end{align*}
\end{proof}

\section{Examples}
\subsection{Linear toral automorphism}
\begin{defin}
A \mdf{linear toral automorphism} is a map $Tx=Ax (mod 1)$ with $A$ a $k\times k$ matrix with integer entries and $\det(A)\neq 0$.

Such an automorphisms is \mdf{hyperbolic} if all eigenvalue for $A$ have $|\lambda|\neq 1$.
\end{defin}
\begin{theorem}
T ergodic $\iff$ no eigenvalue of $A$ is a root of unity.
\end{theorem}
\subsection{Normality of real numbers}
\begin{defin}
$x\in\R$ is \mdf{normal (base b)} if
\begin{itemize}
	\item $x$ has a  unique expansion in that base.
	\item $\forall k\in\{0,1,\dots,b-1\}$
		$$\frac{1}{n}\#\{1\leq i\leq n | x_i=k\}\to\frac{1}{10}\quad\text{as}\;n\to\infty$$
\end{itemize}

$x\in\R$ is \mdf{absolutely normal} if $x$ is normal base b for all $b\geq 2$.
\end{defin}
\begin{theorem}
Almost every $x\in\R$ is absolutely normal.
\end{theorem}

\section{Von Neumann's Ergodic Theorem \& The Adjoint}

\begin{defin}
	Given $T:X\to X$ a measure preserving transformation on a probability space $(X, \mu)$, the \mdf{Koopman operator} is given by
	\[
		\mdf{Uf}\defeq f\circ T
	\]
	for any $f:X\to \R$ measurable.


	Suppose $H$ is a complex Hilbert space with inner product $\langle\cdot, \cdot\rangle$ then a linear operator $U:H\to H$ is an \mdf{isometry} if
	\[
		\norm{Uf}=\norm{f}\quad\forall f\in H
	\]
	where $\norm{f}=\sqrt{\langle f, f\rangle}$.
	Equivalently $\langle Uf, Ug \rangle= \langle f, g\rangle$ for all $f,g \in H$.

	Given a linear operator $U:H\to H$, the \mdf{adjoint} $U*:H\to H$ is the unique bounded linear operator satisfying
	\[
\langle U^\ast f, g \rangle= \langle f, Ug \rangle \quad \forall f, g \in H
	\]

	Let $V\subseteq H$ be a subspace then the \mdf{orthogonal complement} is
	\[
		V^\perp \defeq \left\{ f\in H \rmv \langle f, v \rangle =0 \quad \forall v \in V\right\}
	\]
\end{defin}

\begin{lemma}[Properties of the adjoint]
If $U$ is an isometry then 
\begin{itemize}
	\item $\norm{U^\ast f} \leq \norm{f} \quad \forall f\in H$
	\item $U^\ast U = id $ because
		\[
		\langle U^\ast U f, g \rangle = \langle Uf, Ug\rangle = \langle f, g\rangle \quad \forall f,g \in H
		\]
\end{itemize}
\end{lemma}
\begin{eg}
	\textbf{Computing the adjoint.}
	$X=[0,1]$, $\mu=Leb$, $Tx=2x \mod 1$ and $Uf = f\circ T$ where $U:L^2(X)\toitself$ and our inner product is
	\[
		\langle f, g\rangle \defeq \int_0^1 f \overline{g} \; d\mu
	\]
	\begin{align*}
		\langle U^\ast f, g\rangle & = \langle f, Ug\rangle = \int_0^1 f \overline{Ug} \; dx \\
								   & = \int_0^1 f(x)\overline{g(Tx)}\; dx \\
								   & = \int_0^{\frac{1}{2}} f(x)\overline{g(2x)} \; dx + \int_{\frac{1}{2}}^1 f(x)\overline{g(2x-1)}\; dx \\
								   & = \frac{1}{2}\int_0^1 f\left( \frac{x}{2}\right)\overline{g(x)}\; dx + 
										\frac{1}{2}\int_0^1 f\left( \frac{x+1}{2}\right)\overline{g(x)}\; dx
	\end{align*}
	Hence we can conclude 
	\[
		(U^\ast f)(x) = \frac{1}{2}\left[ f \left( \frac{x}{2}\right) + f \left( \frac{x+1}{2}\right)\right]
	\]
\end{eg}

\begin{prop}
Suppose $U$ is an isometry then
\[
Uf=f \iff U^\ast f = f
\]
\end{prop}

Given a bounded linear operator $A:H\to H$ we can define the \mdf{kernel} to be
\[
	\ker(A) \defeq \left\{ f\in H \rmv Af = 0\right\}
\]
then this a closed subspace in $H$.
Moreover, if $U$ is an isometry then the above proposition tells us that $\ker(U-I)=\ker(U^\ast - I)$.

\textbf{Fact: }
For every closed subspace $V\subseteq H$ we can write $H=V \oplus V^\perp$ and hence
\[
\forall f\in H \quad \exists ! v\in V, w\in V^\perp \; s.t. \; f= v+ w
\]
then we can define \mdf{orthogonal projection} $\pi: H \to V$ by
\[
	\pi(f) = \pi (v+ w) = v
\]

\begin{theorem}[Von Neumann]
If $H$ is a Hilbert space and $U:H\toitself$ is an isometry.
Let $\pi$ denote orthogonal projection into $V=\ker(U-I)$ then
\[
	\frac{1}{n}\sum_{j=0}^{n-1}U^j f \to \pi (f) \quad \text{in }H \quad \text{as} \; n\to \infty
\]
that is
\[
\lim_{n\to\infty}\norm{\frac{1}{n}\sum U^j f - \pi(f)} = 0
\]
\end{theorem}

\begin{proof}
The proof of this is about a page long and definitely warrants a read.
\end{proof}

\begin{cor}
	Given a measure preserving transformation and $Uf=f\circ T$ and $H=L^2(x)$. Then
	\[
		\norm{\frac{1}{n}\sum_{j=0}^{n-1}f\circ T^j - \pi f}_2 \to 0 \quad \text{as} \; n\to\infty
	\]
	If $T$ is ergodic then $\pi f = \int f\; d\mu$.
\end{cor}

\section{Existence of invariant/ergodic measures}
\begin{defin}
Let $\mdf{M(X)}$ be the set of all probability measure on X.

We can view measures as linear functionals on the space of continuous functions as such:
$$\forall f\in C(X)\quad\quad \mdf{\mu(f)}\defeq\int_X f d\mu$$

$\mdf{C(X)^*}\defeq\{\text{bounded linear functionals}\quad w:C(X)\to\R\}$

A linear functional is called \mdf{normalised} if $\int 1 d\mu =1$

A linear functional is called \mdf{positive} if $f\geq 0\implies \int f d\mu \geq 0$
\end{defin}
\begin{theorem}
	Every $\mu\in M(X)$ defines a normalised, positive, bounded, linear functional in $C(X)^*$ defined by $\mu(f)=\int_X f d\mu$.
\end{theorem}
\begin{theorem}[Riesz Representation Theorem]
	Let $w\in C(X)*$ be a bounded linear functional. Suppose that $w$ is positive and normalised. Then $\exists !\mu\in M(X)$ such that $w(f)=\mu(f)$ for all $f\in C(X)$.
\end{theorem}

We would like to give the space $M(X)$ a topology.
Our first idea is the \mdf{strong/norm topology}.
We view $M(X) \subseteq C(X)^\ast$ and inherit the operator norm from $C(X)^\ast$.
That is, given $\mu, \nu \in M(X)$
\[
	d_s(\mu, \nu) \defeq \norm{\mu - \nu} = \sup_{f\in C(X) , \; \norm{f}_\infty=1}\abs{\mu(f)- \lambda(f)} = \sup_{f\in C(X) , \; \norm{f}_\infty=1} \abs{\int f \;d\mu - \int f \; d\lambda}
\]

\begin{note}
	\[
 		\norm{\mu} =1 \quad \forall \mu \in M(X) \subseteq C(X)^\ast
	\]
	since $\abs{\mu(f)}\leq\norm{f}_\infty$ for all $f\in C(X)$ and $\mu(1)=1$.
	Therefore $M(X)$ is a bounded subset of $C(X)^\ast$.
\end{note}

\begin{lemma}
$M(X)$ is closed.
\end{lemma}

\begin{proof}
Suppose we have some sequence $(\mu_n)\subseteq M(X)$ such that $\mu_n\to w\in C(X)^\ast$.
We aim to show that $w=\mu\in M(X)$
We check that the Riesz Representation Theorem is satisfied
\begin{itemize}
	\item Certainly $w\in C(X)^\ast$.
	\item Normalised : $w(1)=\lim_{n\to\infty}\mu_n(1) = \lim_{n\to\infty}1=1$.
	\item Positive: $f\geq 0 \implies \mu_n(f)\geq 0$ for all $n$ and hence $w(f)\geq 0$.
\end{itemize}
\end{proof}

\begin{lemma}
Unfortunately, $M(X)$ is not compact in the strong topology.
\end{lemma}
\begin{proof}
Recall that in a metric space compactness is equivalent to sequential compactness.
So it suffices to find a sequence with no convergent subsequence.
Let $x_1, c_2, \dots \in C$ such that $x_i\neq x_j$ and for all $n$ let $\mu_n=\delta_{x_n}$.

Now take $n\neq m$ we want to show that $\norm{\mu_n - \mu_m} \geq 1$.
For this we define the function 
\[
	f(x)=\frac{d(x, x_n}{d(x, x_n) + d(x, x_m)}
\]
Note since $x_n\neq x_m$ this is well-defined and $f(x_n)=0$ and $f(x_m)=1$.
Moreover, $f$ is continuous and $\norm{f}_\infty = 1$
Therefore $\norm{\delta_{x_n}-\delta_{x_m}}\geq 1$.
\end{proof}

\subsection{Weak $\ast$ topology on $M(X)$}
\begin{defin}
	Let $\mu_n\in M(X)$ and $\mu\in M(X)$.
	We say that $\mu_n\to\mu$ \mdf{weak $\ast$} if
	\[
		\mu_n(f)\to\mu(f) \quad \forall f\in C(X)
	\]
	We can then given $M(X)$ a metric by fixing some countable dense subset $\left\{ f_1, f_2, \dots \right\}\subseteq C(X)$ and defining
	\[
		\mdf{d(\lambda, \mu)}\defeq \sum_{i=1}^{\infty}\frac{1}{2^i}\frac{1}{\norm{f_i}_\infty}\underbrace{\abs{\lambda(f_i)-\mu(f_i)}}_{\leq\norm{f_i}_\infty \int 1 d(\lambda - \mu) \leq \norm{f_i}}\in [0, 1]
	\]
\end{defin}

\begin{prop}
$d$ is a metric.
\end{prop}

\begin{proof}
The difficult thing to prove here is that $\lambda \neq \mu \implies d(\lambda, \mu ) > 0$.
Suppose that we have measures $\lambda \neq \mu$.
By the Riesz Representation Theorem, they must constitute different element of $C(X)^\ast$.
So there is an $f\in C(X)$ such that $\lambda(f)\neq \mu(f)$.
Since the $f_i$ are dense there is some $i$ such that
\[
	\norm{f_i-f}_\infty < \frac{\abs{\lambda(f)-\mu(f)}}{3}
\]
Now
\begin{align*}
	\norm{\lambda(f)-\mu(f)} & \leq \abs{\lambda(f) - \lambda(f_i)} + \abs{\lambda(f_i) - \mu(f_i)} + \abs{\mu(f_i) + \mu(f)} \\
							 & \leq 2 \norm{f_i - f}_\infty + \abs{\lambda(f_i) - \mu(f_i)}\\
							 & < \frac{2\abs{\lambda(f) - \mu(f)}}{3} + \abs{\lambda(f_i) - \mu(f_i)}
\end{align*}
Therefore $\abs{\lambda(f_i)- \mu(f_i)} > \frac{1}{3}\abs{\lambda(f) - \mu(f)} > 0$
So one term of the sum is non-zero and therefore $d(\lambda, \mu) >0$.
\end{proof}

\begin{prop}
$\mu_n \to \mu$ weak $\ast$ $\iff$ $d(\mu_n, \mu)\to 0$ as $n\to\infty$.
\end{prop}

\begin{proof}
Suppose that $\mu_n \to \mu$ weak $\ast$ and choose $\epsilon > 0$.
There exists $M$ such that
\[
	\sum_{i=M}^\infty\frac{1}{2^i}< \frac{\epsilon}{2}
\]
Then 
\[
	d(\mu_n, \mu) \leq \sum_{i=1}^{M}\left[ \frac{1}{2^i}\frac{1}{\norm{f_i}_\infty}\abs{\mu_n(f_i) - \mu(f_i)}\right] + \frac{\epsilon}{2}
\]
Also there is an $N$ such that for $n\geq N$ we can be sure each summand is less than $\frac{\epsilon}{2M}$ since $\mu_n\to \mu$ weak $\ast$ and we only have finitely many $i$ to deal with.
Therefore for any $n\geq N$ we have $d(\mu_n , \mu) \leq \epsilon$.

Conversely, suppose that $d(\mu_n, \mu)\to 0$ then choose $f\in C(X)$ and $\epsilon >0$.
Then there is an $i$ such that $\norm{f_i -f}_\infty < \frac{\epsilon}{3}$.
Also
\[
	\abs{\mu_n(f_i)-\mu(f_i)}\leq 2^i\norm{f_i}d(\mu_n, u) \to 0 \quad \text{as}\; n\to\infty
\]
so there is an $N$ such that for all $n\geq N$ we have $\abs{\mu_n(f_i) - \mu(f_i)} < \frac{\epsilon}{3}$.
Then we can do the normal to trick to show that $\abs{\mu_n - \mu(f)}< \epsilon$.
\end{proof}

\begin{theorem}
$M(X)$ is weak $\ast$ compact.
\end{theorem}

\subsection{Existence of Invariant Measures}
Given $X$ a compact metric space, let $\mathcal{B}$ denote the Borel $\sigma$-algebra and $M(X)$ be defined as before.
Let $T:X\to X$ be a continuous map.
Define
\[
	\mdf{M(X, T)}\defeq \left\{ \mu\in M(X) \rmv T_\ast \mu = \mu\right\}
\]
One can show that for any $f\in C(X)$ we have $T_\ast\mu(f) = \mu(f\circ T)$.
This is proven first for simple functions and then slowly built up.

\begin{theorem}[Krylov-Bogulyvhov]
$M(X, T)\neq \emptyset$.
\end{theorem}

\begin{proof}
We know that $M(X) \neq \emptyset$ (i.e. use a Dirac measure), so we can choose $\sigma \in M(X)$.
Then also note that $T_\ast^j \sigma\in M(X)$ for all $j \geq 1$
Now define a sequence of measures by
\[
	\mu_n \defeq \frac{1}{n}\sum_{j=0}^{n-1}T_\ast^J \sigma
\]
Since $M(X)$ is convex we see that $\mu_n\in M(X)$ for every $n$.
By weak $\ast$ compactness of $M(X)$ we get a convergent subsequence $\mu_{n_k}\to \mu\in M(X)$ weak $\ast$.

Now we need to show that $T^\ast \mu=\mu$ which we do via the Riesz Representation Theorem.
Choose arbitrary $f\in C(X)$.
Notice 
\[
\abs{T_\ast \mu_n(f) - \mu_n(f)} = \abs{ \frac{1}{n}T_\ast^n\sigma(f) - \sigma(f)} = \frac{1}{n}\abs{\sigma(f \circ T^n) - \sigma(f)} \leq \frac{2}{n}\norm{f}_\infty
\]
But then
\[
\abs{\mu_{n_k}(f \circ T) - \mu_{n_k} (f) } = \abs{T_\ast \mu_{n_k} (f) - \mu_{n_k} (f)} \leq \frac{2}{n_k}\norm{f}_\infty
\]
Taking limits on both sides the left goes to $\abs{T_\ast\mu(f) - \mu(f)}$ and the right goes to $0$.
\end{proof}

\begin{prop}
$M(X, T)$ is convex.
\end{prop}

\begin{prop}
$M(X, T)$ is weak $\ast$ compact.
\end{prop}

\begin{proof}
Note it suffices to prove that $M(X, T)$ is closed since $M(X, T)\subseteq M(X)$ and $M(X)$ is compact.
In metric spaces we can just make sure all sequences have limits in $M(X, T)$.
Let $\mu_n\in M(X, T)$ such that $\mu_n \to \mu\in M(X)$ weak $\ast$.

Take $f\in C(X)$ then notice
\[
	T_\ast\mu(f) = \mu(f\circ T) \leftarrow \mu_n (f\circ T) = \mu_n (f) \to \mu(f)
\]
By uniqueness of limits and since $f$ was arbitrary we have that $\mu = T_\ast \mu$ and hence $\mu\in M(X, T)$.
\end{proof}



\subsection{Existence of Ergodic Measures}
\begin{defin}
 Let $Y$ be a convex set then $y\in Y$ is called \mdf{extremal} if
 \[
 \exists y_0, y_1 \in Y \quad \text{and} \quad t\in (0, 1)\; s.t. \quad y = (1-t)y_0 + ty_1 \implies y=y_0=y_1
 \]
\end{defin}

\begin{prop}
$\mu\in M(X, T)$ is extremal $\implies$ $\mu$ is ergodic.
\end{prop}

\begin{proof}
Suppose that $\mu$ is not ergodic so there exists $B\in\mathcal{B}$ such that $T^{-1}B = B$ and $\mu(B)\in [0, 1)$.
Then we let
\[
\mu_0(A)\defeq\frac{\mu(A\cap B)}{\mu(B)} \quad \quad \mu_1(A)\defeq\frac{\mu(A\cap B\stcmp)}{\mu(B\stcmp)}
\]
One can show that these define $T$-invariant measures and satisfy
\[
	\mu(B)\mu_0 + \mu(B\stcmp)\mu_1 = \mu
\]
Note that $\mu(B)$ and $\mu(B\stcmp)$ are both in $(0, 1)$ and $\mu_i\neq \mu$ for either $i$.
Hence $\mu$ cannot be extremal.
\end{proof}

For the opposite direction we need the Radon-Nikodym Theorem.

\begin{defin}
	Given measures $\mu, \nu$ on a measure space $(X, \mathcal{B})$	we say that $\nu$ is \mdf{absolutely continuous} with respect to $\nu$ if
	\[
		B\in\mathcal{B}, \quad \mu(B) = 0 \implies \nu(B) = 0
	\]
\end{defin}

\begin{theorem}[Radon-Nikodym Theorem]
Given a measurable space and measures $\mu, \nu$.
Suppose $\mu$ is $\sigma$-finite and $\nu << \mu$.
Then there is a unique $h:X\to [0, +\infty]$ such that
\[
	\forall B\in \mathcal{B} \quad \nu(B) = \int_B h \; d\mu
\]
Then we write $h=\frac{d\nu}{d\mu}$.
\end{theorem}

\begin{prop}
Given $\mu, \nu\in M(X, T)$ and $\nu << \mu$ write $h=\frac{d\nu}{d\mu}$.
Then $h$ is a $T$-invariant function, i.e. $h \circ T = h$.
\end{prop}

\begin{theorem}
$\mu \in M(X, T)$ is ergodic $\implies$ $\mu$ is extremal.
\end{theorem}

\begin{proof}
Suppose that $\mu = (1-t)\mu_0 + t\mu_1$ for some $\mu_0, \mu_1\in M(X, T)$ and $0 < t < 1$.
We aim to show that in fact $\mu=\mu_0 = \mu_1$.
Notice that for any $B\in\mathcal{B}$ we have that $\mu(B) \geq (1-t)\mu_0$ and hence $\mu_0 << \mu$.
The Radon-Nikodym Theorem gives us a unique $h$ such that
\[
	\mu_0(B) = \int_b h \; d\mu
\]
Notice that $\mu_0$ is a probability measure and hence $\int_X h \; d\mu = 1$.
Also we have seen that $h\circ T = h$ and so by ergodicity we must have that $h$ is constant.
But since $\int_X h \; d\mu=1$ we must have that $h\equiv 1 $ almost everywhere and hence $\mu_0 = \mu$.
\end{proof}

\begin{theorem}
Suppose $T: X \to X$ is a continuous map on a compact metric space.
Then there is a $\mu\in M(X, T)$ which is extremal and hence ergodic.
\end{theorem}

\subsection{Abundance and Uniqueness of Ergodic Measures}
\begin{eg}
	Suppose that $T:X \to X$ is continuous and $x_0\in X$ is a fixed point then $\delta_{x_0}\in M(X, T)$ is an ergodic, $T$-invariant measure.

	Suppose that $x_0$ is a periodic points so that $T^qx_0 = x_0$ then define
	\[
		\mu \defeq \frac{1}{q}\sum_{j=0}^{k-1}\delta_{T^jx_0}
	\]
	Then this is also $T$-invariant and ergodic.
	So the doubling map on $S^1$ has a countable infinity of ergodic, invariant probability measures arising in this way.
\end{eg}

So the doubling map has infinitely many ergodic invariant probability measures.
It also has one ergodic absolutely continuous invariant probability measure (a.c.i.p.) which is absolutely continuous wrt $Leb$, namely the Lebesgue measure itself.
Are there any others?

\begin{prop}
Given $T:X\ to X$ a measure preserving transformation and $\mu, \nu \in M(X, T)$
\[
\nu << \mu \implies \nu = \mu
\]
\end{prop}

\begin{proof}
Choose some arbitrary $B\in \mathcal{B}$, we aim to show that $\nu(B)=\mu(B)$.
By the ergodic theorem, there is a set $E_\mu$ such that $\mu(E_\mu)=1$ and
\[
	\forall x \in E_\mu \quad \frac{1}{n}\sum_{j}\chi_b(T^jx)\to \mu(B)
\]
And likewise there is some set $E_\nu$ with $\nu(E_\nu) = 1$ and
\[
	\forall x \in E_\nu \quad \frac{1}{n}\sum_{j}\chi_b(T^jx)\to \nu(B)
\]
If we can find a point in $E_\mu \cap E_\nu$ then by the uniqueness of limits we are done.

Let $h=\frac{d\nu}{d\mu}$.
Then note
\[
	\nu(E_\nu) = \int_{E_\nu}h\; d\mu = \int_X h \; d\mu = 1
\]
because $E_\mu$ has full measure under $\mu$ and also $\nu(X)=1$.
So we see that $\nu(E_\mu)=\mu(E_\nu)=1$ and hence their intersection has full measure and so is non-empty.
\end{proof}

\begin{cor}
	The doubling map has a unique acip (with respect to $Leb$), namely the Lebesgue measure itself.
\end{cor}

\begin{defin}
	We say a measurable $T:X\to X$ is \mdf{uniquely ergodic} if $\abs{M(X, T)}=1$.
\end{defin}

\begin{note}
	If a transformation $T$ is uniquely ergodic then the unique $T$-invariant measure is certainly extremal in $M(X, T)$ and hence must be ergodic, justifying the name.
\end{note}

\begin{prop}
Irrational rotation $T:\mathbb{T}\to \mathbb{T}, \; x\mapsto x_\alpha$, for $\alpha\in\R\setminus\Q$ are uniquely ergodic.
\end{prop}

\begin{proof}
We're in a compact metric space and so we can use the Riesz representation theorem.
We take some arbitrary $\mu\in M(X, T)$ and aim to show that $\int f\;d\mu = \int f \;dLeb$ for all $f\in C(X)$.
We proceed by a density argument using the space of trigonometric polynomials.

Consider $e^{2\pi i n x}$
\[
	\int e^{2\pi i n x} \; d\mu = \int e^{e\pi i n (Tx)} \; d\mu = e^{2\pi i n \alpha}\int e^{2\pi i n x} \; d\mu
\]
Notice $\alpha\not\in \Q$ and hence so long as $n\neq 0$ then we have $e^{2\pi i n \alpha}\neq 1$.
Therefore $\int e^{2\pi i n x} = 0$.

Now choose some arbitrary trig polynomial $P(x) = \sum_{\abs{n}\leq q}c_n e^{e\pi i n x}$
Then we have just shown that $\int P \dm = c_0 = \int P \; dLeb$.
For arbitrary $f\in C(X)$ we proceed by the usual density argument.
\end{proof}

\begin{theorem}
Given a continuous map $T:X\to X$ on a compact metric space, the following are equivalent:
\begin{enumerate}[label=(\alph*)]
	\item For every $f\in C(X)$ there is some $c=c(f)\in \R$ such that
		\[
			\frac{1}{n}\sum_{j}f\circ T^j \to c\quad\text{uniformly on }X
		\]
	\item For every $f\in C(X)$ there is some $c=c(f)\in \R$ such that
		\[
			\frac{1}{n}\sum_{j}f\circ T^j \to c\quad\text{pointwise on }X
		\]
	\item There is some $\mu\in M(X, T)$ such that
		\[
			\frac{1}{n}\sum_{j}f\circ T^j \to \int f\dm \quad\text{pointwise}\quad\forall f\in C(X)
		\]
	\item $T$ is uniquely ergodic.
\end{enumerate}
\end{theorem}

\begin{proof}
\textit{(a)}$\implies$\textit{(b)}.
Trivial.

\textit{(b)}$\implies$\textit{(c)}.
We let $c(f)$ be as in \textit{(b)} and define $w: C(X) \to \R$ by $w(f) = c(f)$.
Our aim is hence to show that $w$ is a positve, normalised, bounded linear functional and so must correspond to a measure.

Note that $w$ is linear.
$w$ is bounded because 
\[
	\lim_{n\to\infty}\norm{\frac{1}{n}\sum_{j=0}^{n-1}f\circ T^j}_\infty \leq \norm{f}_\infty
\]
by the triangle inequality.
So certainly $w\in C(X)^\ast$.
We also have positivity and normalised fairly trivially.
So by the Riesz Representation theorem there is a $\mu\in M(X)$ such that $w(f) = \int f \dm$.
We just need to show that $\mu$ is $T$-invariant.
\[
	\int f\circ T \dm = \lim_{n\to\infty}\left[\frac{1}{n}\sum_{j=0}^{n-1}(f \circ T) \circ T^j \right]=
	\lim_{n\to\infty}\left[ \frac{1}{n}\sum_{j=0}^{n-1}(f \circ T^j) + \cancelto{0}{\frac{f\circ T^n}{n}} - \cancelto{0}{\frac{f}{n}}\quad\right]
	= \int f \dm
\]

\textit{(c)}$\implies$\textit{(d)}.
We want to show $M(X, T)=\left\{ \mu\right\}$.
Suppose $\nu\in M(X, T)$ so our aim is to show $\nu(f) = \mu(f)$ for all $f\in C(X)$ and then apply the Riesz Representation Theorem.
\[
	\begin{WithArrows}
		\nu(f) &= \int f \; d\nu = \frac{1}{n}\sum_{j=0}^{n-1}\int (f\circ T^j) \; d\nu \quad \text{ since }\nu\in M(X, T) \\
			   &= \lim_{n\to\infty}\frac{1}{n}\sum_{j=0}^{n-1}\int (f\circ T^j) \; d\nu \Arrow{$(1)$} \\
			   &= \int \left(\lim_{n\to\infty} \frac{1}{n}\sum_{j=0}^{n-1} f\circ T^j \right)\; d\nu \Arrow{by \textit{(c)}} \\
			   &= \int (\int f \dm) \; d\nu \Arrow{const over prob measure} \\
			   &= \int f \dm = \mu(f)
	\end{WithArrows}
\]
Note in $(1)$ we switched the limit and the integral.
This is justified because $\frac{1}{n}\sum_{j=0}^{n-1} (f\circ T^j)$ is bounded above by $\norm{f}_\infty$ and converges pointwise so we can use the DCT.

\textit{(d)}$\implies$\textit{(a)}.
Suppose that our unique ergodic measure is called $\mu$ and that \textit{(a)} does not hold.
So in particular \textit{(a)} fails with $c(f) = \int_X f \dm$, i.e. \highlight{there is some $f\in C(X)$} such that
\[
	\frac{1}{n}\sum_{j}f\circ T^j \not \to \int f \dm \quad\text{ uniformly}
\]
Then there are \highlight{sequences $n_k\to \infty$ and $x_k\in X$} such that
\[
	\frac{1}{n_k}\sum_{j=0}^{n_k-1}f(T^jx_k) \not \to \int f \dm
\]
We aim to contradict the uniqueness of $\mu$ so we set 
\[
	\highlight{\nu_k \defeq \frac{1}{n_k}\sum_{j=0}^{n_k-1}\delta_{T^jx_k}}
\]
Note that $\highlight{ \nu_k(f) = \frac{1}{n_k}\sum_{j=0}^{n_k-1}f(T^jx_k)\not\to\mu(f) }$. 
Then $\nu_k\in M(X)$ and $M(X)$ is weak $\ast$ compact so by passing to a subsequence we can assume that $\highlight{\nu_k\to \nu}$ weak $\ast$ for some $\nu\in M(X)$.
By the same proof as the Krylov-Bogulyvhov Theorem $\highlight{\nu\in M(X, T)}$.
It remains to show that $\nu\neq \mu$.
\[
	\nu(f) =  \lim_{k\to\infty}\nu_k(f) \neq \mu (f)
\]
\end{proof}

\section{Shifts}
Most of this is verbatim from Dynamical Systems.
There is something on the cylinder generating the Borel $\sigma$-algebra which is probably worth reading.

\subsection{Bernoulli Measures}
Take $\Sigma^+ \defeq \left\{ 1, \dots, k\right\}^\N$ and let $\sigma$ denote the natural shift map.
Fix some $p=(p_1, \dots, p_k)\in \R^q$ such that each $p_i\geq 0$ and $\sum p_i=1$.
Now we define the measure of cylinders by the formula 
\[
	\mu[y_0, \dots, y_m] \defeq p_{y_0}p_{y_1}\dots p_{y_m}
\]
We claim that this extends to a measure on the Borel $\sigma$-algebra.
This will come to be known as a \mdf{Bernoulli measure}.

\begin{prop}
$\mu$ extends uniquely to a probability measure on $\Sigma^+$.
\end{prop}

\begin{proof}
Define $\mathcal{A}\defeq\left\{ \text{finite unions of cylinder sets}\right\}$.
This is an algebra and it generates the Borel $\sigma$-algebra.
Given $A\in\mathcal{A}$ we can write $A$ uniquely as a finite disjoint union of cylinders $C_i$
\[
	A = \bigcup_{i=1}^p C_i \quad \implies \quad \mu(A) = \sum_{i=1}^p \mu(C_i)
\]
We would like to use Hahn-Kolmogorov Theorem and hence we need to show that given $A_1, A_2, \dots \in \mathcal{A}$ which are disjoint such that $\cup_i A_i\in\mathcal{A}$ then $\mu(\cup_i A_i) = \sum_{i}\mu(A_i)$
But in fact any such union must be finite and hence the condition trivial holds.
So we get a unique probability measure on all of $\Sigma^+$, it is certainly a probability measure because
\[
	\mu(\Sigma^+) = \mu(\bigcup_{i=1}^k \left[ i\right] = \sum_{i=1}^k \mu\left[ i\right] =\sum_{i=1}^k p_i = 1
\]
\end{proof}

\begin{prop}
$\mu$ is $\sigma$-invariant.
\end{prop}

\begin{proof}
Note by the uniqueness on $H-K$ it suffices to prove $\mu(\sigma^{-1}A) = \mu(A)$ for each $A\in\mathcal{A}$.
\end{proof}

\begin{prop}
If $A\in\mathcal{A}$ then there is an $n\geq 0$ such that $\mu(A\cap \sigma^{-n}A)=\mu(A)^2$.
\end{prop}

\begin{theorem}
Bernoulli measures are ergodic.
\end{theorem}

\begin{proof}
This is definitely worth proving.
\end{proof}

\begin{cor}
There are uncountably many distinct ergodic, invariant probability measures on $\Sigma^+$.
\end{cor}

We have the standard semi-conjugacy $\pi: \Sigma_k^+ \to [0, 1)$ from the full one-sided shift on $k$ symbols to $[0, 1)$ given by
\[
	\pi(x_0, x_1, x_2, \dots ) \defeq \sum_{k=0}^{\infty}\frac{x_j}{k^{j+1}}
\]
We can use this to push Bernoulli measures on $\Sigma_k^+$ forward into measures for $[0, 1)$ by
\[
	\pi_\ast \mu(A) \defeq \mu(\pi^{-1}A)
\]
One can easily show that these will be invariant and ergodic measures for $[0, 1)$ with the map $Tx = kx \mod 1$.
But do different Bernoulli measures push forward to different measures on $[0, 1)$?

\begin{proof}
Yes!
Take two Bernoulli measures associated to starting distributions $\mv{p}$ and $\mv{q}$ such that for some specific $j$ we know $p_j \neq q_j$.
Then in the associated Bernoulli measures we notice
\[
\mu_p(\left[ j\right]) = p_j \neq q_j = \mu_q(\left[ j\right])
\]

Take $B\defeq \left[ \frac{j}{k}, \frac{j+1}{k} \right)$.
Then $\pi^{-1}(B) = \left[ j\right]$ and hence
\[
	\pi_\ast \mu_p(B) = p_j \neq q_j = \pi_\ast \mu_q(B)
\]
\end{proof}

\subsection{Markov Measures}
We will need the Perron-Frobenius Theorem.

\begin{theorem}[Perron-Frobenius]
For an aperiodic matrix $B$
\begin{itemize}
	\item $\exists \lambda > 0$ eigenvalue of $B$ such that for all remaining eigenvalues $\lambda' \leq \abs{\lambda}$.
	\item $\lambda$ is simple so there is a unique eigenvector $\mv{v}$ such that $B\mv{v} = \lambda\mv{v}$ and $\sum_{i}v_i=1$.
	\item This $\mv{v}$ is positive, i.e. $v_i > 0$ for every $i$.
	\item If $w$ is another eigenvector then some of its coordinates are positive and others are negative.
\end{itemize}
We call such $\lambda$ the \mdf{maximal eigenvalue}.
\end{theorem}

Consider a subshift of finite type associated to an aperiodic matrix $A\in\left\{ 0, 1\right\}^{k\times k}$.
Choose some $k\times k$ matrix $P$ with non-negative entries such that
\begin{itemize}
	\item $P$ is a \mdf{row-stochastic} if for all $i$, $\sum_{j=1}^k p_{ij} =1$.
	\item $P$ is \mdf{compatible with $A$}, i.e. $p_{ij} >0 \iff A_{ij}=1$.
\end{itemize}

Note that we get the following desirable properties:
\begin{itemize}
	\item $A$ aperiodic $\implies$ $P$ aperiodic since the same choice of $n$ will work.
	\item $(1, 1, \dots, 1)^T$ is an eigenvector with eigenvalue $1$.
	\item By the Perron-Frobenius Theorem, $1$ must be the maximum eigenvalue.
		Therefore, there must be a unique left eigenvector $q$ such that
		\[
			qP = d \quad \sum_{i}q_i = 1\quad q_i >0
		\]
\end{itemize}
We can use this to define a new measure by first defining on cylinder sets
\[
	\mu(\left[ y_0, \dots, y_m \right]) \defeq q_{y_0}P_{y_0 y_1}\dots P_{y_{m-1}y_m}
\]
By the Hahn-Kilogram Theorem this extends to a unique measure on $\Sigma_A^+$ and note
\[
	\mu(\Sigma_A^+)=\mu\left( \bigcup_{i=1}^k [j] \right)=\sum_{j=1}^k \mu( \left[ j\right]) = \sum_{j=1}^k q_j =1
\]
So $\mu$ is a probability measure and $\mu$ is the unique such extension.
We call $\mu$ the \mdf{Markov measure corresponding to $P$}.

\begin{prop}
Markov measures are invariant under the shift map.
\end{prop}

\begin{proof}
\begin{align*}
	\mu(\sigma^{-1}[y_0, \dots, y_m]) &= \mu\left( \cup_{i=1}^k [j, y_0, \dots, y_m]\right) \\
									  &= \sum_{j=1}^k\mu[j, y_0, y_1, \dots, y_m] \\
									  &= \sum_{j=1}^k q_jP_{j y_0} P_{y_0 y_1} \dots P_{y_{m-1} y_m} \\
									  &= \underbrace{\left( \sum_{j=1}^k q_j P_{j y_0}\right)}_{=qy_0}P_{y_0 y_1} \dots P_{y_{m-1} y_m}
									  = \mu[y_0, \dots, y_m]
\end{align*}
\end{proof}

\begin{note}
Markov measure are in fact ergodic but we are not going to prove it.
\end{note}

We can recover Bernoulli measures as a special case of Markov measures.
Given our initial distribution $q=(q_1, \dots, q_k)$ we form the matrix $P$ by
\[
P\defeq
\begin{pmatrix}
	q_1 & \dots & q_k \\
	\vdots & \ddots & \vdots \\
	q_1 & \dots & q_k
\end{pmatrix}
\]
and then we can easily see that $qP=q$ and the corresponding Markov measure coincides with the corresponding Bernoulli measure.

Given any aperiodic matrix $A$ with maximum eigenvalue $\lambda>0$ we know that there are unique vectors $u, v$ such that
\begin{align*}
	Av = \lambda v &\sum_{i}v_i=1 & v_i > 0 \\
	uA^T = \lambda u & \sum_{i}u_i =1 & u_i >0
\end{align*}
Then we define a new matrix $P$ by $P_{ij}\defeq\frac{A_{ij} v_i}{\lambda v_i}$.
We also define a distribution $q_i\defeq\frac{u_i v_i}{c}$ where $c\defeq \sum_{i}u_i v_i$.
Then we can see that $P$ is compatible with $A$ and is now row-stochastic.
Moreover, $qP=q$ and $\sum_{i}q_i = 1$ and $q_i>0$ for all $i$.
The corresponding Markov measure $\mu$ is then called the \mdf{Parry measure}.

\section{Mixing}
\begin{theorem}
Suppose $T:X \to X$ is a measure preserving transformation on $(X,\mathcal{B}, \mu)$.
The following are equivalent:
\begin{enumerate}[label=(\alph*)]
	\item $T$ is ergodic.
	\item For all $A,B\in\mathcal{B}$ we have
		\[
			\frac{1}{n}\sum_{j=0}^{n-1}\mu(T^{-j}A\cap B) \to \mu(A)\mu(B)
		\]
\end{enumerate}
\end{theorem}

\begin{proof}
\textit{(b)}$\implies$\textit{(a)}.
Choose $B\in\mathcal{B}$ such that $T^{-1}B=B$.
Then in the formula take $A=B$ so that $T^{-j}A\cap B=B$.
Then we can see that $\mu(B)=\mu(B)^2$ and hence $\mu(B)\in\left\{ 0, 1\right\}$.

\textit{(a)}$\implies$\textit{(b)}.
We apply the ergodic theorem for $f=\indic{A}$.
We get that
\[
	\frac{1}{n}\sum_{0}^{n-1}\indic{A}\circ T^j \to \mu(A) \quad a.e.
\]
Let's multiply this expression by $\indic{B}$.
\[
	\frac{1}{n}\sum_{j=0}^{n-1}\indic{T^{-j}A\cap B} = \frac{1}{n}\sum_{j=0}^{n-1}(\indic{A}\circ T^j)\indic{B} \to \mu(A)\indic{B}\quad a.e.
\]
Then we integrated both sides and use the dominated convergence theorem since the left hand side is bounded by $1$ which has finite integral.
Hence
\[
	\frac{1}{n}\sum_{j=0}^{n-1}\mu(T^{-j}A\cap B) = \int \frac{1}{n}\sum_{j=0}^{n-1}\indic{T^{-j}A\cap B} \to \int \mu(A) \indic{B} = \mu(A) \mu(B)
\]
\end{proof}

\begin{defin}
	We say that $T$ is \mdf{weak mixing} if
	\[
		\frac{1}{n}\sum_{j=0}^{n-1}\abs{\mu(T^{-j}A\cap B) - \mu(A)\mu(B)} \to 0
	\]
	and say that $T$ is \mdf{(strong) mixing} if
	\[
		\mu(T^{-n}A\cap B)) \to \mu(A)\mu(B)
	\]
	each for all $A, B\in\mathcal{B}$.

	A subset $J\subseteq \N$ is of \mdf{density $d$} if
	\[
		\frac{\abs{J\cap \left\{ 0, 1, \dots , n-1\right\}}}{n}\to d\quad \text{as} \; n\to\infty
	\]
	We say $J$ has
	\begin{itemize}
		\item \mdf{full density} if $d=1$,
		\item \mdf{zero density} if $d=0$, and
		\item \mdf{positive density} if $d>0$.
	\end{itemize}
\end{defin}

Notes on density:
\begin{itemize}
	\item If $d$ exists then certainly $d\in \left[ 0, 1\right]$.
	\item If $J$ has density $d$ then $\N \setminus J$ has density $1-d$.
	\item If $T$ is ergodic and $B\in\mathcal{B}$ is a Borel set then
		\[
			J\defeq\left\{ n \geq 0 \rmv T^n x \in B\right\}\text{ has density }\mu(B) \text{ for a.e. }x
		\]
\end{itemize}

\begin{proof}
\[
	\frac{\abs{J\cap \left\{ 0, 1, \dots, n-1\right\}}}{n}= \frac{1}{n}\sum_{j=0}^{n-1}\indic{B}(T^j x) \to \mu(B)
\]
by the ergodic theorem.
\end{proof}

\begin{lemma}
Let $a_n\in\R$ be a bounded sequence.
The following are equivalent:
\begin{enumerate}
	\item $\lim_{n\to\infty}\frac{1}{n}\sum_{j=0}^{n-1}\abs{a_j} = 0$
	\item There is a subset $J\subseteq\N$ of full density such that $\lim_{n\to\infty, \; n\in J}a_n =0$.
\end{enumerate}
\end{lemma}

\begin{proof}
The proof of this is unreasonably long.
\end{proof}

\begin{cor}
	T is weak-mixing if and only if for all $A,B \in \mathcal{B}$ there is some $J\subseteq\N$ of full density such that
	\[
		\lim_{n\to\infty, \; n\in J}\mu(A\cap T^{-n}B) = \mu(A)\mu(B)
	\]
\end{cor}

\begin{proof}
Take $a_n = \mu(A \cap T^{-n}B) - \mu(A)\mu(B)$ in the above lemma.
\end{proof}

\begin{theorem}
$T$ is weak mixing $\implies$ $T\times T$ is ergodic (in fact weak mixing).
Note $T\times T$ is taking place under the product measure on the product $\sigma$-algebra.
\end{theorem}

\begin{cor}
Rotations of the circle are not weak mixing.
\end{cor}

\begin{proof}
It suffices to check that $T\times T$ is not ergodic and hence we will try and find a non-constant invariant function for $T\times T$.
Let's take
\[
	f(x,y)\defeq e^{2\pi i x} e^{-2\pi i y}
\]
then $f:\mathbb{T}\times\mathbb{T}\to \C$ is measurable in $L^\infty$ and is not constant.
Now notice
\[
	f(x+ \alpha, y + \alpha) = e^{2\pi i x} e^{-2\pi i y} e^{2 \pi i \alpha} e^{-2\pi i \alpha} = f(x, y)
\]
and hence $f\circ (T\times T) = f$.
\end{proof}

We now know
\begin{figure}[H]
	\centering
	\begin{tikzcd}[outer sep = 3pt]
		\text{Mixing} \arrow[r, Rightarrow, bend left] &
		\text{Weak mixing} \arrow[r, Rightarrow, bend left] \arrow[l, "\text{Hard to prove}", Rightarrow, bend left, negate, red]&
		\text{Ergodic} \arrow[l, "\text{irrational rotation}", Rightarrow, bend left, negate, red ]
	\end{tikzcd}
\end{figure}

\begin{prop}
The shift map $\sigma: \Sigma_+ \to \Sigma_+$ with a Bernoulli measure is mixing.
\end{prop}

\begin{proof}
When we proved ergodicity we say that for sufficiently large $n$
\[
	\mu(A\cap \sigma^{-n}A) = \mu(A)^2
\]
It is an exercise to show that the same argument gives $\forall A, A'\in\mathcal{A}$
\[
	\mu(A\cap \sigma^{-n}A') = \mu(A) \mu(A') \quad \forall n \text{ sufficiently large}
\]
Then given arbitrary $B, B'\in\mathcal{B}$ we can find $A, A' \in \mathcal{A}$ such that $\mu(A \symd B) < \epsilon)$ and $\mu(A' \symd B') < \epsilon$.
Then we can see
\[
	\abs{\mu(B\cap \sigma^{-n}B' - \mu(B)\mu(B')} \leq \abs{\mu(A \cap \sigma^{-n}A') - \mu(A) \mu(A')} + 4\epsilon \to 0
\]
\end{proof}

\subsection{Correlation Function / Covariance}
We would like to say something about the rate of mixing and to this end we define the following:
\begin{defin}
	The \mdf{correlation function / covariance} is
	\[
		\Cov(f, g\circ T^n)\defeq \int_X (f)(g\circ T^n) \dm - \int_X f\dm \int_X g\circ T^n \dm
	\]
	We will also need the following for a function $f:\Sigma^+ \to \R$
	\[
		\lip(f)\defeq\sup_{x, y \in \Sigma^+, \; x\neq y} \frac{\abs{f(x)-f(y)}}{d(x, y)}
	\]
\end{defin}

Given $T:X \to X$ a measure preserving transformation then
\[
\begin{WithArrows}
	Cov(f, g\circ T^n) & = \int_X f \cdot g \circ T^n \dm - \int_X f \dm \int_X g\circ T^n \dm	\\
					   & = \int_X \left( f - \int_X f \dm\right)g\circ T^n \dm \Arrow{Koopman operator} \\
					   & = \int_X \left( f - \int_X f \dm\right)U^ng \dm \Arrow{$L\defeq U^\ast$}\\
					   & = \int_X L^n  \left(f - \int_X f \dm\right) g \dm
\end{WithArrows}
\]

Now notice the following about $L$
\begin{enumerate}
	\item \[
			\int L 1 \overline{g} = \int 1 \left( \overline{g}\circ T\right) = \int \overline{g}\circ T = \int \overline{g} = \int 1\overline{g}
		\]
		since $T$ is measure preserving.
		Hence $L1 = 1$.
	\item \[
			\int Lf = \int Lf \; 1 = \int (f)(1 \circ T) = \int f\; 1 = \int f
		\]
		and hence
		\[
L^n\int f \dm = \int f \dm = \int L^n f \dm
		\]
\end{enumerate}
Therefore we can conclude that
\[
	\Cov (f, g \circ T^n) = \int_X\left( L^n f - \int L^n f \right)g \dm
\]

So by applying the Cauchy-Schwarz inequality we can see
\begin{prop}
For all $f, g\in L^2$, and $T$ a measure preserving transformation
\[
	\abs{\Cov(f, g \circ T^n)} \leq \norm{L^n f - \int L^n f}_2 \norm{g}_2
\]
\end{prop}

\begin{prop}
\[
	\norm{f - \int f}_\infty \leq \lip(f) \diam(X)
\]
for $f: X \to \R$ Lipschitz on a metric space $(X, d)$ where
\[
	\diam(X) \defeq \sup_{x, y \in X} d(x, y)
\]
\end{prop}

\begin{proof}
Choose some $x\in X$.
Suppose $f(x) > \int f$.
Then there must be some $y$ such that $f(y) \leq \int f$.
Hence $f(x) - f(y) \geq f(x) - \int f > 0$.
Similarly if $f(x) < \int f$.
So we have
\begin{align*}
	\abs{f(x) - \int f} & \leq \sup_{y\in X} \abs{f(x) - f(y)} \\
						& \leq \sup_{y\in X} \lip(f) d(x, y) \\
						& \leq \lip(f) \diam(X)
\end{align*}
\end{proof}

Now take some Bernoulli measure $\mu$ associated to a distribution $(p_1, \dots , p_k)$.
One can show, using a similar method as for the doubling map that
\begin{prop}
For any $f\in L^2(X)$ we have
\[
	(Lf)(x)= \sum_{j=1}^{k}p_j f(jx)
\]
where $jx = (j, x_0, x_1, \dots )$.
\end{prop}

\begin{lemma}
Given $f:\Sigma^+ \to \R$ Lipschitz we have
\[
	\lip(L^n f) \leq \frac{1}{2^n}\lip(f)
\]
\end{lemma}

\begin{proof}
Note because of our choice of metric we have that $d(jx, jy) = \frac{1}{n}d(x, y)$ for any $x, y\in\Sigma^+$.
\begin{align*}
	\abs{(Lf)(x)-(Lf)(y)} & \leq \sum_{j=1}^{k}p_j \abs{f(jx) - f(jy)} \leq \sum_{j=1}^{k} p_j \lip(f) d(jx, jy) \\
						  & = \sum_{j=1}^{k} p_j \lip(f) \frac{1}{2}d(x, y) = \frac{1}{2}\lip(f) d(x, y) \sum_{j=1}^{k}p_j = \frac{\lip(f)}{2}d(x,y)
\end{align*}
Then the result follows by induction.
\end{proof}

Then we get our big theorem on mixing

\begin{theorem}
On $\Sigma^+$ with a Bernoulli measure, we have exponential mixing rates.
That is given $f:\Sigma^+ \to \R$ Lipschitz, $g\in L^2(\Sigma^+)$ and $n\geq 1$ we have
\[
	\abs{\Cov(f, g \circ \sigma^n)} \leq \left( \frac{1}{2}\right)^n \lip(f) \norm{g}_2
\]
\end{theorem}

\begin{proof}
\begin{align*}
	\abs{\Cov(f, g \circ \sigma^n)} &\leq \norm{L^nf - \int_{\Sigma^+}L^nf \dm}_\infty \norm{g}_2 \leq \lip(L^nf)\underbrace{\diam(\Sigma^+)}_{=1}\norm{g}_2 \\
									&= \lip(L^n f) \norm{g}_2 \leq \left( \frac{1}{2}\right)^n\lip(f) \norm{g}_2
\end{align*}
\end{proof}

It was an exercise to show the same thing for $g\in L^1(\Sigma^+)$, using the 1-norm on $g$.
It was also an exercise to estimate mixing rates for the doubling map when $f$ is Lipschitz or H\"older continuous.

\section{Entropy}

\subsection{Conditional Expectation}
Suppose we have a probability space $(X, \mathcal{B}, \mu)$ and a sub $\sigma$-algebra $\mathcal{A} \subseteq \mathcal{B}$.
Our aim is to define a conditional expectation operator
\[
	\mathbb{E}(\bullet | \mathcal{A}) : L^1(X, \mathcal{B}, \mu) \to L^1(X, \mathcal{A}, \mu)
\]
with the property that $\mathbb{E}(\bullet | \mathcal{A})^2=\mathbb{E}(\bullet | \mathcal{A})$
We would also like that
\[
	\int_A \mathbb{E}(f | \mathcal{A}) \dm = \int_A f \dm \quad \forall A \in \mathcal{A}
\]

\begin{enumerate}
	\item First take $f\in L^1(X, \mathcal{B}, \mu)$ such that $f\geq 0$ and define
		\[
			\nu(A0 \defeq \int_A f \dm \quad \forall A\in\mathcal{A}
		\]
		This gives us a measure on $\mathcal{A}$.
		Notice that $\restr{\mu}{\mathcal{A}}$ is also a measure on $\mathcal{A}$ and in fact we claim that $\nu << \restr{\mu}{\mathcal{A}}$.
		So by the Radon-Nikodym theorem there is a unique $h_f \geq 0$ such that $h_f$ is $\mathcal{A}$-measurable and
		\[
			\nu(A) = \int_A h_f \dm \quad \forall A\in\mathcal{A}
		\]
		We then define $\mdf{\mathbb{E}(f | \mathcal{A})} \defeq h_f$ which has the property that it is $\mathcal{A}$-measurable and has the same integral as $f$.
		This can be seen as the ``best approximation to $f$ that is $\mathcal{A}$-measurable".
	\item For arbitrary $f\in L^1(X, \mathcal{B}, \mu)$ we write $f= f^+ - f^-$ and then define
		\[
			\mdf{\mathbb{E}(f | \mathcal{A})} \defeq \mathbb{E}(f^+ | \mathcal{A}) - \mathbb{E}(f^- | \mathcal{A})
		\]
\end{enumerate}

\begin{prop}
\label{prop:condexpec}
The conditional expectation operator is uniquely defined by the properties
\begin{enumerate}[label=(\roman*)]
	\item $\mathbb{E}(f | \mathcal{A})$ is $\mathcal{A}$ measurable.
	\item $\int_A \mathbb{E}(f | \mathcal{A})\dm = \int_A f \dm$ for all $A\in \mathcal{A}$.
\end{enumerate}
\end{prop}

\begin{lemma}
The conditional expectation operator is linear.
\end{lemma}

Note that if $f$ is already $\mathcal{A}$ measurable then $\mathbb{E}(f | \mathcal{A})$ because $f$ already satisfies $\textit{(i)}$ and $\textit{(ii)}$.
Also if $\mathcal{A}= \left\{ X, \emptyset\right\}$ then 
\[
	\mathbb{E}(f | \left\{ X, \emptyset\right\} ) = \int_X f \dm
\]

\begin{prop}
Given probability spaces $(X, \mathcal{B}_X, \mu_X)$ and $(Y, \mathcal{B}_Y, \mu_Y)$ and a measure preserving transformation $T:X \to Y$, let $\mathcal{A}\subseteq \mathcal{B}_Y$ be a sub $\sigma$-algebra.
Then for all $f\in L^1(Y, \mathcal{B}_Y, \mu_Y)$ we have
\[
	\expg{f }{ \mathcal{A}} \circ T = \expg{f \circ T}{T^{-1}\mathcal{A}}
\]
\end{prop}

\begin{proof}
It suffices to check that the left hand side is $T^{-1}\mathcal{A}$-measurable and that
\[
	\int_{T^{-1}A} \expg{f}{\mathcal{A}}\circ T = \int_{T^{-1}A} f \circ T \quad \quad \forall A \in \mathcal{A}
\]
\begin{enumerate}[label=(\roman*)]
	\item Note that $\expg{f}{\mathcal{A}}$ is already $\mathcal{A}$-measurable and hence the left hand side is $T^{-1}\mathcal{A}$-measurable.
	\item 
		\[
			\begin{WithArrows}
				\int_{T^{-1}A} \expg{f}{\mathcal{A}} \circ T \; d\mu_X & = \int_X (\indic{A} \expg{f}{\mathcal{A}}) \circ T \; d\mu_X \Arrow{T m.p.t.} \\
																	   & = \int_Y \indic{A} \expg{f}{\mathcal{A}} \; d\mu_Y \\
																	   & = \int_A \expg{f}{\mathcal{A}} \; d\mu_Y \Arrow{$A\in\mathcal{A}$} \\
																	   & = \int_A f \; d\mu_Y \Arrow{same tricks} \\
																	   & = \int_X (\indic{A} f ) \circ T \; d\mu_X \\
																	   & = \int_{T^{-1}A} f\circ T \; d\mu_X
			\end{WithArrows}
		\]
\end{enumerate}
\end{proof}

We can also prove the following by the same technique.

\begin{prop}
Given a measure space $(X, \mathcal{B}, \mu)$ and nested sub $\sigma$-algebras $\widetilde{\mathcal{A}}\subseteq\mathcal{A}\subseteq\mathcal{B}$
\[
	\expg{\expg{f}{\mathcal{A}}}{\widetilde{\mathcal{A}}} = \expg{f}{\widetilde{\mathcal{A}}} \quad \quad \forall f \in L^1(X, \mathcal{B}, \mu)
\]
\end{prop}

Given a measure preserving transformation $T: X \to X$ we know that $U^\ast U = I$ but it is an exercise to show that
\[
	UU^\ast = \expg{\bullet}{T^{-1}\mathcal{B}}
\]



\subsection{An Ergodic Aside}
Recall the ergodic theorem.
Suppose we have a measure preserving transformation $T:X \to X$ on a measure space $(X, \mathcal{B}, \mu)$ which is not necessarily ergodic.
IF $f\in L^1(X)$ then almost everywhere we have
\[
	\frac{1}{n}\sum_{j=0}^{n-1}f\circ T^j \to \widetilde{f}
\]
where $\widetilde{f}\in L^1(X)$ and $\int \widetilde{f}\dm = \int f \dm$ and $\widetilde{f}\circ T = \widetilde{f}$.

When $f\in L^2(X)$ we found out that $\widetilde{f}=\pi f$ where $\pi$ is the orthogonal projection onto
\[
\ker(U - I) = \left\{ T\text{-invariant functions} \right\}
\]

We would like a formula for arbitrary $f\in L^1(X)$.
To this end define the set
\[
	I \defeq \left\{ B\in \mathcal{B} \rmv T^{-1}B = B\right\}
\]
then we can see that $I\subseteq\mathcal{B}$ is a sub $\sigma$-algebra of $\mathcal{B}$.

\begin{theorem}
In the ergodic theorem $\widetilde{f}= \expg{f}{I}$ for all $f\in L^1(X)$.
\end{theorem}

\begin{proof}
We do the same style of proof.
\begin{enumerate}[label=(\roman*)]
	\item We wish to show that $\widetilde{f}$ is $I$-measurable.
		We know that it is $\mathcal{B}$-measurable so take $U\in\mathcal{B}(R)$ then
		\[
			\widetilde{f}^{-1}(U) = (\widetilde{f}\circ T)^{-1}(U) = T^{-1}(\widetilde{f}^{-1}U)
		\]
		and hence $\widetilde{f}^{-1}U\in I$.
	\item Take any $A\in I$ then we have that
		\[
			\frac{1}{n}\sum_{j=0}^{n-1}(\indic{A} f ) \circ T^j = \indic{A} \cdot \frac{1}{n}\sum_{j=0}^{n-1}f \circ T^j \to \indic{A} \widetilde{f}\quad a.e.
		\]
		because $A\in I$ means we can move the indicator function outside.
		So we can conclude
		\[
			\int_A f = \int_X \indic{A} f = \int_X \indic{A}\widetilde{f} = \int_A \widetilde{f}
		\]
\end{enumerate}
\end{proof}

\subsection{Defining Entropy}
The motivation for a definition of entropy is as a vehicle to distinguish between dynamical systems. First we need to know how tell when two systems are identical.
\begin{defin}
	Two probability spaces with measure preserving transformations, $(X,\mathcal{B},\mu,T),(Y,\mathcal{C}, \nu, S)$ are \mdf{measure-theoretically isomorphic} if there exists a bijection $\pi:B\to C$ where $B\in\mathcal{B}$ and $C\in\mathcal{C}$ such that
	\begin{itemize}
		\item $\mu(B)=\nu(C)=1$
		\item $T(B)\subseteq B, S(C)\subseteq C$
		\item $\pi: B\to C$ and $\pi^{-1}:C\to B$ are measure preserving transformations
		\item $\pi\circ T = S\circ\pi$
		\begin{figure}[H]
			\centering
			\begin{tikzcd}
				X \arrow[r, "T"] \arrow[d, "\pi"'] & X \arrow[d, "\pi"] \\
				Y \arrow[r, "S"'] & Y
			\end{tikzcd}
		\end{figure}	
	\end{itemize}
	
Assume $\msrspc$ is a probability space and $\alpha=\{A_i\}$ a countable collection of subsets $A_i\subseteq B$.
\begin{itemize}
	\item We say $\alpha$ is a \mdf{partition} of $X$ if $\cup A_i$ = X and $A_i\cap A_j=\emptyset$ up to measure 0.
	\item The \mdf{join} of two partitions $\alpha,\beta$ is the partition $\alpha\vee\beta$ of all possible intersections $A_i\cap B_j$.
	\item A countable partition $\beta$ is a \mdf{refinement} of $\alpha$ if every element of $\alpha$ is a union of element of $\beta$ and write $\alpha\leq\beta$.
	\item $\alpha,\beta$ are \mdf{independent} if $\mu(A\cap B)=\mu(A)\mu(B)$ for all $A\in\alpha, B\in\beta$.
	\item The \mdf{information of a partition} $\alpha$ is
		$$I(\alpha)\defeq-\sum_{A\in\alpha}\indic{A}\log(\mu(A))$$
		where $I(\alpha):X\to[0,\infty]$.
	\item The \mdf{entropy of a partition} $\alpha$ is
		$$H(\alpha)\defeq\int_X I(\alpha)d\mu=-\sum_{A\in\alpha}\mu(A)\log(\mu(A))$$
		using the convention $0\cdot\log(0)=0$.
	\item The \mdf{expectation given a partition} is
		$$\expg{\cdot}{\alpha}\defeq\expg{\cdot}{\sigma(\alpha)}$$
	\item The \mdf{conditional probability} of $B\in\mathcal{B}$ given $\alpha$ is
		$$\probg{B}{\alpha}\defeq\expg{\indic{B}}{\alpha}$$
\end{itemize}
Suppose that $\mathcal{C}$ is a sub $\sigma$-algebra of $\mathcal{B}$.
\begin{itemize}
	\item The \mdf{conditional information of $\alpha$ given $\mathcal{C}$} is
		$$\infog{\alpha}{\mathcal{C}}\defeq - \sum_{A\in\alpha}\indic{A}\log(\mu\gvn{A}{\mathcal{C}})$$
	where $\mu\gvn{A}{\mathcal{C}}\defeq\expg{\indic{A}}{\mathcal{C}}$
	\item The \mdf{conditional entropy of $\alpha$ given $\mathcal{C}$} is
			$$\entrg{\alpha}{\mathcal{C}}\defeq\int_X\infog{\alpha}{\mathcal{C}}d\mu$$	
\end{itemize}
\end{defin}
We have the following desirable properties:
\begin{itemize}
	\item If $\alpha$ and $\beta$ are independent then $I(\alpha\vee\beta)=I(\alpha)+I(\beta)$.
	\item If $\alpha=\{X\}$ then $I(\alpha)=0$ so $H(\alpha)=0$.
	\item If $T$ is a measure preserving transformation then $H(T^{-1}\alpha)=H(\alpha)$.
	\item Given $A\in\alpha$, $\restr{\expg{f}{\alpha}}{A}=\frac{\int_A f \;d\mu}{\mu(A)}$ and hence
		$$\expg{f}{\alpha}=\sum_{A\in\alpha}\indic{A}\frac{\int_A f \;d\mu}{\mu(A)}$$
	\item Conditional probability and expectation are constant on partition elements.
	\item For $A\in\alpha$, 
		$$\restr{\probg{B}{\alpha}}{A}=\restr{\expg{\indic{B}}{\alpha}}{A}=\frac{\int_A \indic{B} d\mu}{\mu(A)}=\frac{\mu(A\cap B}{\mu(A)}$$
	\item If $\mathcal{C}=\{X,\emptyset\}$ then $\infog{\alpha}{\mathcal{C}}=I(\alpha)$ and $\entrg{\alpha}{\mathcal{C}}=H(\alpha)$.
	\item If $g\geq 0$ is $\sigma(\alpha)$-measurable then $\expg{fg}{\sigma(\alpha)}=g\cdot\expg{f}{\sigma(\alpha)}$.
	\item If $T$ is a measure preserving transformation then $\infog{T^{-1}\alpha}{T^{-1}\mathcal{C}}=\infog{\alpha}{\mathcal{C}}\circ T$.
	\item Integrating this gives $\entrg{T^{-1}\alpha}{T^{-1}\mathcal{C}}=\entrg{\alpha}{\mathcal{C}}$.
	\item $\alpha\leq\beta\implies\infog{\alpha}{\beta}=0$.
\end{itemize}


\begin{prop}
	$$\entrg{\alpha}{\mathcal{C}}=-\int_X \sum_{A\in\alpha}\mu\gvn{A}{\mathcal{C}}\log(\mu\gvn{A}{\mathcal{C}})d\mu$$	
\end{prop}

\begin{proof}
Use the definition and then write out $\infog{\alpha}{\mathcal{C}}$ explicit.
Use the monotone convergence theorem to swap sum and limit.
Change the integrand to the conditional expectation on $\mathcal{C}$ and then pull out the $\log(\mu\gvn{A}{\mathcal{C}})$ term.
\end{proof}

\begin{lemma}[Basic Identity]
	Given $\alpha,\beta,\gamma$ partitions of $X$ then
	$$\infog{\alpha\vee\beta}{\gamma}=\infog{\alpha}{\gamma}+\infog{\beta}{\alpha\vee\gamma}$$
	$$\entrg{\alpha\vee\beta}{\gamma}=\entrg{\alpha}{\gamma}+\entrg{\beta}{\alpha\vee\gamma}$$
\end{lemma}
\begin{proof}
This can be proved by showing equality on each partition element.
That is, choose $A\in \alpha$, $B\in \beta$, $C\in\gamma$ and then show equality for $x\in A \cap B \cap C$.
\end{proof}
\begin{cor}
	$$\beta\leq\gamma\implies\infog{\alpha\vee\beta}{\gamma}=\infog{\alpha}{\gamma}$$
\end{cor}
\begin{cor}[Monotonicity of information of entropy]
	$$\alpha\leq\beta\implies\infog{\alpha}{\gamma}\leq\infog{\beta}{\gamma}$$
\end{cor}
\begin{cor}[Anti-monotonicity of entropy]
	$$\beta\leq\gamma\implies\entrg{\alpha}{\beta}\geq\entrg{\alpha}{\gamma}$$
\end{cor}
\begin{cor}
We have the two following properties as well:
\begin{itemize}
	\item $\entrg{\alpha}{\gamma}\leq H(\alpha)$ (because always $\gamma\geq\{X,\emptyset\}$)
	\item $\entrg{\alpha\vee\beta}{\gamma}\leq\entrg{\alpha}{\gamma}+\entrg{\beta}{\gamma}$
\end{itemize}
\end{cor}
So far this does not encapsulate any dynamics of the system and so we must use these concepts to arrive at a definition of entropy which depends on the transformation. For convenience define the following set:
$$\mathcal{P}\defeq\{\alpha\;\text{countable partitions}\;|H(\alpha) < \infty\}$$
Now choose $\alpha\in\mathcal{P}$. Then we define the following:
$$\mdf{H_n(\alpha)}\defeq H(\alpha^n)\quad\text{where}\quad\alpha^n\defeq\bigvee_{j=0}^{n-1}T^{-j}\alpha$$
This has the convenient property that $H_{n+m}(\alpha)\leq H_n(\alpha)+H_m(\alpha)$, i.e. these $H_n$ form a sub-additive sequence $\R$-valued sequence and hence the limit $\mdf{h(T,\alpha)}\defeq\lim_{n\to\infty}\frac{1}{n}H_n(\alpha)$ exists. We call this the \mdf{entropy of T relative to $\alpha$}.
We can then define the \mdf{entropy of T} by taking the supremum:
$$\mdf{h(T)}\defeq\sup_{\alpha\in\mathcal{P}}h(T,\alpha)$$
Having done all this work, this had better be a measure-theoretic isomorphism invariant.

\begin{theorem}
Given two measure preserving transformations $(X, \mathcal{B}, \mu, T)$ and $(Y, \mathcal{C}, \nu, S)$ that are measure-theoretically isomorphic
\[
	h(T) = h(S)
\]
\end{theorem}

\begin{proof}
Choose $\alpha\in\mathcal{P}_Y$ some countable partition over $Y$ then we get a partition over $C$.
Then $\pi^{-1}\alpha\in\mathcal{P}_B$ which gives us a countable partition for $X$, $\pi^{-1}\alpha\in\mathcal{P}_X$.

\[
	H_\mu\left( \bigvee_{j=0}^{n-1} T^{-j}(\pi^{-1}\alpha)\right) = H_\mu\left( \pi^{-1}\bigvee_{j=0}^{n-1} S^{-j}\alpha\right)
	= H_\nu\left(\bigvee_{j=0}^{n-1} S^{-j}\alpha \right)
\]
with the last equality holding because $\pi$ is measure preserving.
Dividing by $n$ and taking $n\to\infty$ we see that
\[
	h_\mu(T, \pi^{-1}\alpha) = h_\nu(S, \alpha)
\]
Then taking supremums over $\alpha\in\mathcal{P}_Y$ corresponds to taking supremums over $\pi^{-1}\alpha \in \mathcal{P}_X$.
Hence $h_\mu(T) = h_\nu(S)$.
\end{proof}

\begin{note}
Given a countable partition $\alpha$ we can apparently write
\[
	h(T, \alpha) = \int \infog{\alpha}{\bigvee_{j=1}^\infty T^{-j}\alpha}\dm
\]
\end{note}

\begin{theorem}
\begin{enumerate}[label=(\roman*)]
	\item For any $k\geq 0$ we have $h(T^k) = kh(T)$.
	\item Moreover if $T$ is invertible with measure preserving inverse then for any $k\in \Z$ we have $k(T^k)=\abs{k}h(T)$.
\end{enumerate}
\end{theorem}

\subsection{Abramov and Sinai Theorems}

\begin{defin}
	Given countable partitions $\alpha_n$ and a $\sigma$-algebra $\mathcal{B}$, we say $\alpha_n\uparrow\mathcal{B}$ if
	\[
		\alpha_1 \leq \alpha_2 \leq \dots \quad \text{and} \quad \sigma\left( \bigcup_{n=1}^\infty \alpha_n\right)=\mathcal{B}
	\]
\end{defin}

\begin{lemma}
Given countable partitions $\alpha, \beta \in \mathcal{P}$
\[
	h(T, \alpha) \leq h(T, \beta) + \entrg{A}{B}
\]
\end{lemma}

\begin{proof}
By monotonicity we can see that $H(\alpha^n) \leq H(\alpha^n \vee \beta^n) = H(\beta^n) + \entrg{\alpha^n}{\beta^n}$
\[
\begin{WithArrows}
	\entrg{\alpha^n}{\beta^n} & = \entrg{\bigvee_{j=0}^{n-1}T^{-j}\alpha}{\bigvee_{i=0}^{n-1} T^{-i}\beta} 
	\leq \sum_{j=0}^{n-1}\entrg{T^{-j}\alpha}{\bigvee_{i=0}^{n-1} T^{-i}\beta}\Arrow{anti-monotonicity}\\
							  & \leq \sum_{j=0}^{n-1}\entrg{T^{-j}\alpha}{T^{-j}\beta} \Arrow{m.p.t}\\
							  & = n \entrg{\alpha}{\beta}
\end{WithArrows}
\]
\end{proof}

\begin{lemma}
Suppose $\alpha, \alpha_n\in\mathcal{P}$ and $\alpha_n\uparrow\mathcal{B}$ then
\[
	\lim_{n\to\infty}\entrg{\alpha}{\alpha_n} = 0
\]
\end{lemma}

\begin{theorem}[Abramov's Theorem]
Given a probability space $(X, \mathcal{B}, \mu)$ and $T:X \to X$ a measure preserving transformation, let $\alpha_n\in\mathcal{P}$ such that $\alpha_n\uparrow\mathcal{B}$.
Then
\[
	\lim_{n\to\infty}h(T, \alpha_n) = h(T)
\]
\end{theorem}

\begin{proof}
Well certainly $h(T) \geq \limsup_{n\to\infty} h(T, \alpha_n)$.
Conversely let $\alpha\in\mathcal{P}$, then by the first Lemma we see
\[
	h(T, \alpha) \leq h(T, \alpha_n) + \entrg{\alpha}{\alpha_n}
\]
Then taking $n\to \infty$ and using the second lemma we get
\[
	h(T, \alpha) \leq \liminf_{n\to\infty} h(T, \alpha_n)
\]
\end{proof}

\begin{defin}
	If $T$ is invertible and $\bigvee_{j=-n}^n T^{-j}\alpha\uparrow\mathcal{B}$ then we say $\alpha$ is a \mdf{generator}.

	If $\bigvee_{j=0}^n T^{-j}\alpha\uparrow\mathcal{B}$ then we say $\alpha$ is a \mdf{strong generator}.
\end{defin}

\begin{theorem}[Sinai's Theorem]
Let $(X, \mathcal{B}, \mu)$ be a probability space and $T:X \to X$ a measure preserving transformation.
Given $\alpha\in\mathcal{P}$, if
\begin{enumerate}
	\item $T$ is invertible and $\alpha$ is a generator OR
	\item $\alpha$ is a strong generator
\end{enumerate}
then $h(T) = h(T, \alpha)$.
\end{theorem}

\begin{proof}
We do case 2 but the proof for 1 is similar.
By Abramov's Theorem we have that
\[
	h\left( T, \bigvee_{j=-n}^n T^{-j}\alpha\right)\to h(T) \quad \text{as }n\to\infty
\]
But then
\[
\begin{WithArrows}
	H\left( \bigvee_{i=0}^{k-1}T^{-i}\left( \bigvee_{j=-n}^n T^{-j}\alpha\right)\right)
	&= H(T^n\alpha \vee \dots \vee T^{-(n+k+1)} ) \\
	&= H(T^n(\alpha \vee \dots \vee T^{-(2n+k+1)})) \Arrow{$T^{-1}$ m.p.t} \\
	&= H\left( \bigvee_{j=0}^{2n+k-1} T^{-j}\alpha\right)
\end{WithArrows}
\]
So then
\[
	h\left(T, \bigvee_{j=-n}^n T^{-j}\alpha \right)=\lim_{k\to\infty}\frac{2n+k-1}{k}\frac{1}{2n+k-1}H\left( \bigvee_{j=0}^{2n+k-1} T^{-j}\alpha\right)=h(T,\alpha)
\]
\end{proof}

\begin{eg}
	\textbf{The doubling map has entropy $\log(2)$}

	Define $\alpha = \left\{ \left[ 0, \frac{1}{2}\right], \left[ \frac{1}{2}, 1\right]\right\}$.
	Then notice
	\[
		\alpha^n = \left\{ \left[ \frac{j}{2^n}, \frac{j+1}{2^n}\right] \rmv 0 \leq j \leq 2^n -1\right\}
	\]
	is a strong generator and $h(T) = h(T, \alpha)$

	But then
	\[
		H(\alpha^n) = \sum_{j=1}^{2^n}-\frac{1}{2^n}\log\left(\frac{1}{2^n}\right)=\log(2^n) = n\log(2)
	\]
	and hence $h(T, \alpha) = \log(2)$.
\end{eg}

\begin{theorem}[Entropy of a Markov Measure]
Given an aperiodic $k\times k$ matrix $A$ with entries on $\left\{ 0, 1\right\}$ take some row-stochastic matrix $P$ compatible with $A$.
By the Perron-Frobenius Theorem we get a unique vector $\mv{q}$ with positive entries who sum to $1$ and such that $\mv{q}P = \mv{q}$.
The entropy of the shift map under the corresponding Markov measure is then
\[
	h(T) = -\sum_{i, j=1}^{k}q_i P_{ij}\log P_{ij}
\]
\end{theorem}

\begin{proof}
We're going to define a strong generator
\[
	\alpha\defeq\left\{ \left[ 1\right], \left[ 2\right], \dots, \left[ k\right]\right\}
\]
Then $\sigma^{-1}\alpha = \left\{ \left[ \ast, 1\right], \left[ \ast, 2\right], \dots, \left[ \ast, k\right]\right\}$.
So therefore we see that
\[
	\alpha^n = \left\{ \left[ x_0, x_1, \dots, x_{n-1} \right] \subseteq \Sigma^+\right\}
\]
that is the set of all admissible cylinders of length $k$.
Therefore
\begin{align*}
	H(\alpha^{n+1}) & = - \sum_{x_0=1}^{k}\dots\sum_{x_n=1}^{k}q_{x_0}P_{x_0x_1}\dots P_{x_{n-1}x_n}(\log q_{x_0} + \log P_{x_0x_1} + \dots + \log P_{x_{n-1}x_n})\\
					& = - \sum_{x_0=1}^{k}\dots\sum_{x_n=1}^{k}q_{x_0}P_{x_0x_1}\dots P_{x_{n-1}x_n} \log q_{x_0} \\
					& \quad \quad - \sum_{x_0=1}^{k}\dots\sum_{x_n=1}^{k}q_{x_0}P_{x_0x_1}\dots P_{x_{n-1}x_n} (\log P_{x_0x_1} + \dots + \log P_{x_{n-1}x_n})
\end{align*}
Now we're going to deal with each of these terms in turn.
In the first term we can use the row-stochasticty to peel off each sum
\[
\begin{WithArrows}
& \quad \sum_{x_0=1}^{k}\dots\sum_{x_n=1}^{k}q_{x_0}P_{x_0x_1}\dots P_{x_{n-1}x_n} \log q_{x_0} \Arrow{Sum last $P$ term}\\
& =    \sum_{x_0=1}^{k}\dots\sum_{x_{n-1}=1}^{k}q_{x_0}P_{x_0x_1}\dots P_{x_{n-2}x_{n-1}} \log q_{x_0}\\
& \vdots \\
& = \sum_{x_0=1}^k q_{x_0} \log q_{x_0} = \sum_{i=1}^k q_i \log q_i
\end{WithArrows}
\]
For the second term we split of each $\log$ terms up.
We cancel sums from the right as far as possible using row-stochasticity and then cancel from the left using $\mv{q}P=\mv{q}$.
For example
\[
	\begin{WithArrows}
		& \quad \sum_{x_0=1}^{k}\dots\sum_{x_n=1}^{k}q_{x_0}P_{x_0x_1}\dots P_{x_{n-1}x_n} \log P_{x_1x_2} \Arrow{row-stochastic} \\
		& = \sum_{x_0}\sum_{x_1}\sum_{x_2}q_{x_0}P_{x_0x_1}P_{x_1x_2}\log P_{x_1x_2} \Arrow{$\mv{q}P=\mv{q}$}\\
		& = \sum_{x_1}\sum_{x_2}q_{x_1}P_{x_1x_2}\log P_{x_1x_2} \\
		& = \sum_{i, j=1}^{k}q_i P_{ij}\log P_{ij}
	\end{WithArrows}
\]
We can do the same thing for each other $\log$ term to see
\[
	H(\alpha^{n+1}) = - \sum_{i=1}^{k}q_i \log(q_i) - n \sum_{i, j =1}^{k}q_i P_{ij} \log(P_{ij})
\]
So when we divide by $n+1$ and take $n\to\infty$ we get
\[
	h(T) = h(T, \alpha) = \lim_{n\to\infty}\frac{H(\alpha^{n+1})}{n+1} = - \sum_{i, j=1}^{k}q_i P_{ij} \log P_{ij}
\]
\end{proof}

\end{document}
