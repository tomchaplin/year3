\documentclass[11pt]{article}

%{{{ Packages
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{amsthm}
\usepackage{mathdots}
\usepackage[thicklines]{cancel}
\renewcommand{\CancelColor}{\color{red}}
\usepackage[dvipsnames]{xcolor}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage{microtype}
\usepackage{mathabx}
\usepackage{tikz-cd}
\usepackage{float}
\usepackage{silence}
\WarningFilter{mdframed}{You got a bad break}
\setlength{\parindent}{0pt}
%}}}
%{{{ Custom commands
% Nice maths commands
\newcommand{\defeq}{:=}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|}
%\renewcommand{\dots}{...}
\newcommand{\msrspc}{\ensuremath{(X,\mathcal{B},\mu)}}
\newcommand{\symd}{\triangle}
\newcommand{\indic}[1]{\mathbbm{1}_{#1}}
\newcommand\restr[2]{{% we make the whole thing an ordinary symbol
  \left.\kern-\nulldelimiterspace % automatically resize the bar with \right
  #1 % the function
  \vphantom{\big|} % pretend it's a little taller at normal size
  \right|_{#2} % this is the delimiter
  }}
\newcommand{\relmiddle}[1]{\mathrel{}\middle#1\mathrel{}}
\newcommand{\rmv}{\relmiddle|}
\newcommand{\toitself}{\righttoleftarrow}
\newcommand{\stcmp}{^{\mathsf{c}}}

% Spaces
\newcommand{\ktor}{\mathbb{T}^k}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}

% Ergodic Theory
\newcommand{\gvn}[2]{\ensuremath{\left(#1\;|\;#2\right)}}
\newcommand{\expg}[2]{\ensuremath{\mathbb{E}\gvn{#1}{#2}}}
\newcommand{\infog}[2]{\ensuremath{I\gvn{#1}{#2}}}
\newcommand{\entrg}[2]{\ensuremath{H\gvn{#1}{#2}}}
\newcommand{\probg}[2]{\ensuremath{\mathbb{P}\gvn{#1}{#2}}}
%}}}
%{{{ Environments
% Definitions environment
\newenvironment{defin}
	{\begin{mdframed}[backgroundcolor=white, roundcorner=5pt, linewidth=1pt]}
	{\end{mdframed}}
\newcommand{\mdf}[1]{{\color{red} #1}}

% Important notes environment
\newenvironment{note}
	{\begin{mdframed}[backgroundcolor=white, linecolor=red, roundcorner=5pt, linewidth=1pt]\bfseries{Note:}\normalfont}
	{\end{mdframed}}

% Examples enviornmnet
\definecolor{mylg}{rgb}{0.9,0.9,0.9}
\newenvironment{eg}
	{\begin{mdframed}[backgroundcolor=mylg,roundcorner=5pt,linewidth=0pt]\bfseries{Example:}\normalfont}
	{\end{mdframed}}

% Theorem enviornment
\newtheorem{prop}{Proposition}[section]
\newtheorem{theorem}[prop]{Theorem}
\newtheorem{lemma}[prop]{Lemma}
\newtheorem{cor}[prop]{Corollary}
%}}}
%{{{ Document metadata
\title{Ergodic Theory Notes}
\author{}
\date{}
%}}}

\begin{document}
\maketitle

\section{Basic Definitions}

For this section we fix a probability space \msrspc and we have a transformation $T:X\to X$ which is measurable in our probability space.

\begin{defin}
	
We say $T$ is a \mdf{measure preserving transformation (m.p.t.)} or $\mu$ is a \mdf{$T$-invariant measure} if 
$$\mu(T^{-1}B)=\mu(B)\quad\forall B\in\mathcal{B}$$

The \mdf{push forward of $\mu$ by $T$} is defiend to be
	$$T_*\mu(B)=\mu(T^{-1}B)\quad\forall B \in\mathcal{B}$$

We say a measure $\mu$ is \mdf{regular} if $\forall B\in\mathcal{B}$ we have $\forall\epsilon >0 \exists U\subseteq X$ open such that
$$B\subseteq U \quad \text{and} \quad \mu(U) < \mu(B) + \epsilon$$

An m.p.t $T$ is said to be \mdf{ergodic} if
$$\forall B\in\mathcal{B},\; T^{-1}B=B \implies \mu(B)=0\;\text{or}\;1$$

\end{defin}

\section{Facts on Fourier Series}
Suppose $f\in L_1(\ktor)$ then we can define the \mdf{Fourier coefficients} by
$$\hat{f}(n)=\int_{\ktor}f(x)e^{-2\pi in\cdot x}dx\quad\forall n\in\Z^k$$
\begin{theorem}[Fej\'er's Theorem]
The average of the partial Fourier sums converges uniformly to $f$, i.e.
$$\frac{1}{N}\sum_{k=0}^{N-1}S_kf\to f\quad\text{uniformly}$$
\end{theorem}
\begin{theorem}[Riemann-Lebesgue Lemma]
	For all $f\in L_1(\ktor)$, $$\lim_{\abs{n}\to\infty}\hat{f}(n)=0$$
\end{theorem}
\begin{theorem}[Reisz-Fisher Theorem]
Define $S_Nf(x)=\sum_{\abs{n}\leq N}\hat{f}(n)e^{2\pi i (n\cdot x)}$
then $S_nf\to f$ in $L^2$ for all $f\in L^2(\ktor)$.
\end{theorem}
\begin{cor}
If $f\in L^2(\ktor)$ and $\hat{f}(n)=0\;\forall n\in\Z^k\setminus\{0\}$, then $f$ is constant.
\end{cor}
\begin{theorem}
Given $f\in L^2$ which is $T$-invariant
$$\hat{f}(n)=\lim_{N\to\infty}\int (S_N f)(Tx)e^{-2\pi i n\cdot x}$$
\end{theorem}
\section{Criteria for measure preserving}
\begin{theorem}
Given $T:X\to X$ on a probability space $(X,\mu)$, the following are equivalent:
\begin{enumerate}
	\item $T$ is m.p.t
	\item $\int f\circ T d\mu = \int f d\mu \quad\quad \forall f\in L_1(X)$.\end{enumerate}
\end{theorem}
Recall the space $L_1(X)=\left\{ f:x\to\R : \text{measurable}\quad \norm{f}_1 \defeq\int \abs{f} d\mu < \infty \right\}$

\begin{theorem}
Given $T:X\to X$ on a probability space $(X,\mu)$, the following are equivalent:
\begin{enumerate}
	\item $T$ is m.p.t
	\item $\int f\circ T d\mu = \int f d\mu \quad\quad \forall f\in C(X)$.
\end{enumerate}
\end{theorem}

So we see that in fact it suffices to check that $T$ does not affect the integral of any continuous function $f$.
However, we can extend this further using the density of trigonometric polynomials in the space of continuous functions. 
First, we need to define a trigonometric polynomial in arbitrary dimension on the $k$-torus $X=\ktor$ with $\mu=leb$ and $\mathcal{B}=Borel$.
\begin{defin}
	$P:\ktor\to\ktor$ is a \mdf{trigonometric polynomial} if for some $N\geq 1$ and $c_n\in\C$ we can write
	$$P(x)=\sum_{\abs{n}\leq N} c_n e^{2\pi in\cdot x}$$
	where $n=(n_1,\dots,n_k)\in\Z^k, x=(x_1,\dots,x_k), \abs{n}=\abs{n_1}+\dots+\abs{n_k}$.
\end{defin}
\begin{note}
	\[
		\int_{\ktor} e^{2\pi n\cdot x} dx =
		\begin{cases}
			1 & \text{if} \; n=0\\
			0 & \text{if} \; n\neq 0
		\end{cases}
	\]
	and hence
	$$\int_{\ktor}P = c_0$$
\end{note}

\begin{theorem}
Given $T:\ktor\to \ktor$ continuous and denoting by $\mu$ the Lebesgue measure.
\begin{enumerate}
	\item $T$ is m.p.t
	\item $\int P\circ T d\mu = \int P d\mu \quad\quad \forall\; \text{trigonometric polynomials}\; P$. 
\end{enumerate}
\end{theorem}

\section{Criteria for Ergodicity}
First another few definitions.
\begin{defin}
Given $A,B\subseteq X$, their \mdf{symmetric difference} is
$$A\symd B\defeq (A \setminus B)\cup (B \setminus A)$$

A function $f$ is \mdf{$T$-invariant} if $f \circ T = f$ a.e.

A function $f$ is \mdf{constant} if $\exists c\in\R$ such that $f(x) = c$ almost everywhere.
\end{defin}

\begin{theorem}
Given a measure preserving transformation $T:X\to X$ and some $1\leq p\leq\infty$. TFAE:
\begin{enumerate}
	\item T is ergodic.
	\item For all $f$ measurable $f$ invariant $\iff$ $f$ constant.
	\item For all $f\in L^p(X)$, $f$ invariant $\iff$ $f$ constant.
\end{enumerate}
\end{theorem}
\begin{note}
To check that $T$ is ergodic it suffices to show that all invariant $L^2$ functions have zero Fourier coefficients away from zero.
\end{note}
To this end we present the following formula for computing the Fourier coefficients of invariant $L^2$ functions.
\begin{theorem}
Given $f\in L^2$ which is invariant
$$\hat{f}(n)=\lim_{N\to\infty}\int (S_Nf)(Tx)e^{-2\pi i n\cdot x}dx$$
\end{theorem}

\section{Theorems using Measure Preserving}
\begin{theorem}[Poincar\'e Recurrence Theorem]
	Given a probability space \msrspc and $T:X\to X$ measure preserving. Then
	$$\mu\{x\in B : T^nx\in B \; \text{infinitely often}\,\}=\mu(B)\quad \forall B\in\mathcal{B}$$
\end{theorem}
\section{Theorems using Ergodicity}
\begin{theorem}[Pointwise Ergoic Theorem - Birkhoff 1931]
	Given a measure space \msrspc and a measure preserving transformation $T:X \to X$ and $f\in L^1(X)$. Then $\exists f^*\in L^1(X)$ invariant such that
	$$\frac{1}{n}\sum_{j=0}^{n-1}f\circ T^j \to f^* \; a.e.\quad \text{and} \quad \int f^* = \int f$$
\end{theorem}
Note this does not actually need ergodicity. However, if we additionally assume ergodicity we can prove the following stronger result.
\begin{cor}
Given a probability space \msrspc, $T$ measure preserving and ergodic, $f\in L^1(x)$, then
$$\underbrace{\frac{1}{n}\sum_{j=0}^{n-1}f\circ T^j }_{\text{Time average}}\to \underbrace{\int f d\mu \; a.e.}_{\text{Space average}}$$
\end{cor}
\begin{theorem}[Mean Ergodic Theorems]
	$1\leq p < \infty$, $T$ measure preserving theorem, $f\in L^p(X)$. Define $f^*\defeq \lim_{n\to\infty} \frac{1}{n}\sum_{j=0}^{n-1}f\circ T^j$ almost everywhere. Then
	$$\lim_{n\to\infty}\frac{1}{n}\sum_{j=0}^{n-1}f\circ T^j = f^*$$
	in $L^p$.
\end{theorem}

\begin{proof}
\textbf{Special Case: }$1\leq p < \infty$ but $f\in L^\infty(X)$.

Then by the ergodic theorem and the DCT with dominator $2^p\norm{f}^p$ we have
\[
	\abs{\frac{1}{n}\sum_{j=0}^{n-1}f\circ T^j - f*}^p \to 0
\]

\textbf{General Case: }Take $f\in L^p$

Given $\epsilon >0$ then there is a $g\in L^\infty$ such that $\norm{f-g}_p < \frac{\epsilon}{3}$.
Then we get $f^\ast$ associated to $f$ and $g^\ast$ associated to $g$.
Then $(f-g)^\ast$ is associated to $f-g$ and $(f-g)^\ast=f^\ast - g^\ast$.
By a previous proposition we can see

\[
	\norm{f^\ast-g^\ast}_p = \norm{(f-g)^\ast}_p \leq \norm{f-g}_p < \frac{\epsilon}{3}
\]
Also since $g\in L^\infty$ there must be an $N$ such that
\[
	n\geq N \implies \norm{\frac{1}{n}\sum_{j=0}^{n-1}g\circ T^j - g^\ast}_p < \frac{\epsilon}{3}
\]
Then
\begin{align*}
	\norm{\frac{1}{n}\sum_{j=0}^{n-1}f\circ T^j - f^\ast}_p & \leq \norm{\frac{1}{n}\sum_{j=0}^{n-1}(f-g)\circ T^j}_p +  \underbrace{\norm{\frac{1}{n}\sum_{j=0}^{n-1}g\circ T^j - g^\ast}_p}_{<\epsilon/3 \text{ for }n\geq N} + \underbrace{\norm{g^\ast - f^\ast}}_{<\epsilon/3} \\
															& \leq \frac{1}{n}\sum_{j=0}^{n-1}\norm{(f-g)\circ \cancelto{\text{m.p.t}}{T^j}}_p + \frac{2\epsilon}{3}\\
															& = \norm{f-g} + \frac{2\epsilon}{3} < \epsilon
\end{align*}
\end{proof}

\section{Examples}
\subsection{Linear toral automorphism}
\begin{defin}
A \mdf{linear toral automorphism} is a map $Tx=Ax (mod 1)$ with $A$ a $k\times k$ matrix with integer entries and $\det(A)\neq 0$.

Such an automorphisms is \mdf{hyperbolic} if all eigenvalue for $A$ have $|\lambda|\neq 1$.
\end{defin}
\begin{theorem}
T ergodic $\iff$ no eigenvalue of $A$ is a root of unity.
\end{theorem}
\subsection{Normality of real numbers}
\begin{defin}
$x\in\R$ is \mdf{normal (base b)} if
\begin{itemize}
	\item $x$ has a  unique expansion in that base.
	\item $\forall k\in\{0,1,\dots,b-1\}$
		$$\frac{1}{n}\#\{1\leq i\leq n | x_i=k\}\to\frac{1}{10}\quad\text{as}\;n\to\infty$$
\end{itemize}

$x\in\R$ is \mdf{absolutely normal} if $x$ is normal base b for all $b\geq 2$.
\end{defin}
\begin{theorem}
Almost every $x\in\R$ is absolutely normal.
\end{theorem}

\section{Von Neumann's Ergodic Theorem \& The Adjoint}

\begin{defin}
	Given $T:X\to X$ a measure preserving transformation on a probability space $(X, \mu)$, the \mdf{Koopman operator} is given by
	\[
		\mdf{Uf}\defeq f\circ T
	\]
	for any $f:X\to \R$ measurable.


	Suppose $H$ is a complex Hilbert space with inner product $\langle\cdot, \cdot\rangle$ then a linear operator $U:H\to H$ is an \mdf{isometry} if
	\[
		\norm{Uf}=\norm{f}\quad\forall f\in H
	\]
	where $\norm{f}=\sqrt{\langle f, f\rangle}$.
	Equivalently $\langle Uf, Ug \rangle= \langle f, g\rangle$ for all $f,g \in H$.

	Given a linear operator $U:H\to H$, the \mdf{adjoint} $U*:H\to H$ is the unique bounded linear operator satisfying
	\[
\langle U^\ast f, g \rangle= \langle f, Ug \rangle \quad \forall f, g \in H
	\]

	Let $V\subseteq H$ be a subspace then the \mdf{orthogonal complement} is
	\[
		V^\perp \defeq \left\{ f\in H \rmv \langle f, v \rangle =0 \quad \forall v \in V\right\}
	\]
\end{defin}

\begin{lemma}[Properties of the adjoint]
If $U$ is an isometry then 
\begin{itemize}
	\item $\norm{U^\ast f} \leq \norm{f} \quad \forall f\in H$
	\item $U^\ast U = id $ because
		\[
		\langle U^\ast U f, g \rangle = \langle Uf, Ug\rangle = \langle f, g\rangle \quad \forall f,g \in H
		\]
\end{itemize}
\end{lemma}
\begin{eg}
	\textbf{Computing the adjoint.}
	$X=[0,1]$, $\mu=Leb$, $Tx=2x \mod 1$ and $Uf = f\circ T$ where $U:L^2(X)\toitself$ and our inner product is
	\[
		\langle f, g\rangle \defeq \int_0^1 f \overline{g} \; d\mu
	\]
	\begin{align*}
		\langle U^\ast f, g\rangle & = \langle f, Ug\rangle = \int_0^1 f \overline{Ug} \; dx \\
								   & = \int_0^1 f(x)\overline{g(Tx)}\; dx \\
								   & = \int_0^{\frac{1}{2}} f(x)\overline{g(2x)} \; dx + \int_{\frac{1}{2}}^1 f(x)\overline{g(2x-1)}\; dx \\
								   & = \frac{1}{2}\int_0^1 f\left( \frac{x}{2}\right)\overline{g(x)}\; dx + 
										\frac{1}{2}\int_0^1 f\left( \frac{x+1}{2}\right)\overline{g(x)}\; dx
	\end{align*}
	Hence we can conclude 
	\[
		(U^\ast f)(x) = \frac{1}{2}\left[ f \left( \frac{x}{2}\right) + f \left( \frac{x+1}{2}\right)\right]
	\]
\end{eg}

\begin{prop}
Suppose $U$ is an isometry then
\[
Uf=f \iff U^\ast f = f
\]
\end{prop}

Given a bounded linear operator $A:H\to H$ we can define the \mdf{kernel} to be
\[
	\ker(A) \defeq \left\{ f\in H \rmv Af = 0\right\}
\]
then this a closed subspace in $H$.
Moreover, if $U$ is an isometry then the above proposition tells us that $\ker(U-I)=\ker(U^\ast - I)$.

\textbf{Fact: }
For every closed subspace $V\subseteq H$ we can write $H=V \oplus V^\perp$ and hence
\[
\forall f\in H \quad \exists ! v\in V, w\in V^\perp \; s.t. \; f= v+ w
\]
then we can define \mdf{orthogonal projection} $\pi: H \to V$ by
\[
	\pi(f) = \pi (v+ w) = v
\]

\begin{theorem}[Von Neumann]
If $H$ is a Hilbert space and $U:H\toitself$ is an isometry.
Let $\pi$ denote orthogonal projection into $V=\ker(U-I)$ then
\[
	\frac{1}{n}\sum_{j=0}^{n-1}U^j f \to \pi (f) \quad \text{in }H \quad \text{as} \; n\to \infty
\]
that is
\[
\lim_{n\to\infty}\norm{\frac{1}{n}\sum U^j f - \pi(f)} = 0
\]
\end{theorem}

\begin{proof}
The proof of this is about a page long and definitely warrants a read.
\end{proof}

\begin{cor}
	Given a measure preserving transformation and $Uf=f\circ T$ and $H=L^2(x)$. Then
	\[
		\norm{\frac{1}{n}\sum_{j=0}^{n-1}f\circ T^j - \pi f}_2 \to 0 \quad \text{as} \; n\to\infty
	\]
	If $T$ is ergodic then $\pi f = \int f\; d\mu$.
\end{cor}

\section{Existence of invariant/ergodic measures}
\begin{defin}
Let $\mdf{M(X)}$ be the set of all probability measure on X.

We can view measures as linear functionals on the space of continuous functions as such:
$$\forall f\in C(X)\quad\quad \mdf{\mu(f)}\defeq\int_X f d\mu$$

$\mdf{C(X)^*}\defeq\{\text{bounded linear functionals}\quad w:C(X)\to\R\}$

A linear functional is called \mdf{normalised} if $\int 1 d\mu =1$

A linear functional is called \mdf{positive} if $f\geq 0\implies \int f d\mu \geq 0$
\end{defin}
\begin{theorem}
	Every $\mu\in M(X)$ defines a normalised, positive, bounded, linear functional in $C(X)^*$ defined by $\mu(f)=\int_X f d\mu$.
\end{theorem}
\begin{theorem}[Reisz Representation Theorem]
	Let $w\in C(X)*$ be a bounded linear functional. Suppose that $w$ is positive and normalised. Then $\exists !\mu\in M(X)$ such that $w(f)=\mu(f)$ for all $f\in C(X)$.
\end{theorem}

We would like to give the space $M(X)$ a topology.
Our first idea is the \mdf{strong/norm topology}.
We view $M(X) \subseteq C(X)^\ast$ and inherit the operator norm from $C(X)^\ast$.
That is, given $\mu, \nu \in M(X)$
\[
	d_s(\mu, \nu) \defeq \norm{\mu - \nu} = \sup_{f\in C(X) , \; \norm{f}_\infty=1}\abs{\mu(f)- \lambda(f)} = \sup_{f\in C(X) , \; \norm{f}_\infty=1} \abs{\int f \;d\mu - \int f \; d\lambda}
\]

\begin{note}
	\[
 		\norm{\mu} =1 \quad \forall \mu \in M(X) \subseteq C(X)^\ast
	\]
	since $\abs{\mu(f)}\leq\norm{f}_\infty$ for all $f\in C(X)$ and $\mu(1)=1$.
	Therefore $M(X)$ is a bounded subset of $C(X)^\ast$.
\end{note}

\begin{lemma}
$M(X)$ is closed.
\end{lemma}

\begin{proof}
Suppose we have some sequence $(\mu_n)\subseteq M(X)$ such that $\mu_n\to w\in C(X)^\ast$.
We aim to show that $w=\mu\in M(X)$
We check that the Reisz Representation Theorem is satisfied
\begin{itemize}
	\item Certainly $w\in C(X)^\ast$.
	\item Normalised : $w(1)=\lim_{n\to\infty}\mu_n(1) = \lim_{n\to\infty}1=1$.
	\item Positive: $f\geq 0 \implies \mu_n(f)\geq 0$ for all $n$ and hence $w(f)\geq 0$.
\end{itemize}
\end{proof}

\begin{lemma}
Unfortunately, $M(X)$ is not compact in the strong topology.
\end{lemma}
\begin{proof}
Recall that in a metric space compactness is equivalent to sequential compactness.
So it suffices to find a sequence with no convergent subsequence.
Let $x_1, c_2, \dots \in C$ such that $x_i\neq x_j$ and for all $n$ let $\mu_n=\delta_{x_n}$.

Now take $n\neq m$ we want to show that $\norm{\mu_n - \mu_m} \geq 1$.
For this we define the function 
\[
	f(x)=\frac{d(x, x_n}{d(x, x_n) + d(x, x_m)}
\]
Note since $x_n\neq x_m$ this is well-defined and $f(x_n)=0$ and $f(x_m)=1$.
Moreover, $f$ is continuous and $\norm{f}_\infty = 1$
Therefore $\norm{\delta_{x_n}-\delta_{x_m}}\geq 1$.
\end{proof}

\subsection{Weak $\ast$ topology on $M(X)$}
\begin{defin}
	Let $\mu_n\in M(X)$ and $\mu\in M(X)$.
	We say that $\mu_n\to\mu$ \mdf{weak $\ast$} if
	\[
		\mu_n(f)\to\mu(f) \quad \forall f\in C(X)
	\]
	We can then given $M(X)$ a metric by fixing some countable dense subset $\left\{ f_1, f_2, \dots \right\}\subseteq C(X)$ and defining
	\[
		\mdf{d(\lambda, \mu)}\defeq \sum_{i=1}^{\infty}\frac{1}{2^i}\frac{1}{\norm{f_i}_\infty}\underbrace{\abs{\lambda(f_i)-\mu(f_i)}}_{\leq\norm{f_i}_\infty \int 1 d(\lambda - \mu) \leq \norm{f_i}}\in [0, 1]
	\]
\end{defin}

\begin{prop}
$d$ is a metric.
\end{prop}

\begin{proof}
The difficult thing to prove here is that $\lambda \neq \mu \implies d(\lambda, \mu ) > 0$.
Suppose that we have measures $\lambda \neq \mu$.
By the Reisz Representation Theorem, they must constitute different element of $C(X)^\ast$.
So there is an $f\in C(X)$ such that $\lambda(f)\neq \mu(f)$.
Since the $f_i$ are dense there is some $i$ such that
\[
	\norm{f_i-f}_\infty < \frac{\abs{\lambda(f)-\mu(f)}}{3}
\]
Now
\begin{align*}
	\norm{\lambda(f)-\mu(f)} & \leq \abs{\lambda(f) - \lambda(f_i)} + \abs{\lambda(f_i) - \mu(f_i)} + \abs{\mu(f_i) + \mu(f)} \\
							 & \leq 2 \norm{f_i - f}_\infty + \abs{\lambda(f_i) - \mu(f_i)}\\
							 & < \frac{2\abs{\lambda(f) - \mu(f)}}{3} + \abs{\lambda(f_i) - \mu(f_i)}
\end{align*}
Therefore $\abs{\lambda(f_i)- \mu(f_i)} > \frac{1}{3}\abs{\lambda(f) - \mu(f)} > 0$
So one term of the sum is non-zero and therefore $d(\lambda, \mu) >0$.
\end{proof}

\begin{prop}
$\mu_n \to \mu$ weak $\ast$ $\iff$ $d(\mu_n, \mu)\to 0$ as $n\to\infty$.
\end{prop}

\begin{proof}
Suppose that $\mu_n \to \mu$ weak $\ast$ and choose $\epsilon > 0$.
There exists $M$ such that
\[
	\sum_{i=M}^\infty\frac{1}{2^i}< \frac{\epsilon}{2}
\]
Then 
\[
	d(\mu_n, \mu) \leq \sum_{i=1}^{M}\left[ \frac{1}{2^i}\frac{1}{\norm{f_i}_\infty}\abs{\mu_n(f_i) - \mu(f_i)}\right] + \frac{\epsilon}{2}
\]
Also there is an $N$ such that for $n\geq N$ we can be sure each summand is less than $\frac{\epsilon}{2M}$ since $\mu_n\to \mu$ weak $\ast$ and we only have finitely many $i$ to deal with.
Therefore for any $n\geq N$ we have $d(\mu_n , \mu) \leq \epsilon$.

Conversely, suppose that $d(\mu_n, \mu)\to 0$ then choose $f\in C(X)$ and $\epsilon >0$.
Then there is an $i$ such that $\norm{f_i -f}_\infty < \frac{\epsilon}{3}$.
Also
\[
	\abs{\mu_n(f_i)-\mu(f_i)}\leq 2^i\norm{f_i}d(\mu_n, u) \to 0 \quad \text{as}\; n\to\infty
\]
so there is an $N$ such that for all $n\geq N$ we have $\abs{\mu_n(f_i) - \mu(f_i)} < \frac{\epsilon}{3}$.
Then we can do the normal to trick to show that $\abs{\mu_n - \mu(f)}< \epsilon$.
\end{proof}

\begin{theorem}
$M(X)$ is weak $\ast$ compact.
\end{theorem}

\subsection{Existence of Invariant Measures}
Given $X$ a compact metric space, let $\mathcal{B}$ denote the Borel $\sigma$-algebra and $M(X)$ be defined as before.
Let $T:X\to X$ be a continuous map.
Define
\[
	\mdf{M(X, T)}\defeq \left\{ \mu\in M(X) \rmv T_\ast \mu = \mu\right\}
\]
One can show that for any $f\in C(X)$ we have $T_\ast\mu(f) = \mu(f\circ T)$.
This is proven first for simple functions and then slowly built up.

\begin{theorem}[Krylov-Bogulyvhov]
$M(X, T)\neq \emptyset$.
\end{theorem}

\begin{prop}
$M(X, T)$ is convex.
\end{prop}

\begin{prop}
$M(X, T)$ is weak $\ast$ compact.
\end{prop}

\begin{proof}
Note it suffices to prove that $M(X, T)$ is closed since $M(X, T)\subseteq M(X)$ and $M(X)$ is compact.
In metric spaces we can just make sure all sequences have limits in $M(X, T)$.
Let $\mu_n\in M(X, T)$ such that $\mu_n \to \mu\in M(X)$ weak $\ast$.

Take $f\in C(X)$ then notice
\[
	T_\ast\mu(f) = \mu(f\circ T) \leftarrow \mu_n (f\circ T) = \mu_n (f) \to \mu(f)
\]
By uniqueness of limits and since $f$ was arbitrary we have that $\mu = T_\ast \mu$ and hence $\mu\in M(X, T)$.
\end{proof}

\subsection{Existence of Ergodic Measures}
\begin{defin}
 Let $Y$ be a convex set then $y\in Y$ is called \mdf{extremal} if
 \[
 \exists y_0, y_1 \in Y \quad \text{and} \quad t\in (0, 1)\; s.t. \quad y = (1-t)y_0 + ty_1 \implies y=y_0=y_1
 \]
\end{defin}

\begin{prop}
$\mu\in M(X, T)$ is extremal $\implies$ $\mu$ is ergodic.
\end{prop}

\begin{proof}
Suppose that $\mu$ is not ergodic so there exists $B\in\mathcal{B}$ such that $T^{-1}B = B$ and $\mu(B)\in [0, 1)$.
Then we let
\[
\mu_0(A)\defeq\frac{\mu(A\cap B)}{\mu(B)} \quad \quad \mu_1(A)\defeq\frac{\mu(A\cap B\stcmp)}{\mu(B\stcmp)}
\]
One can show that these define $T$-invariant measures and satisfy
\[
	\mu(B)\mu_0 + \mu(B\stcmp)\mu_1 = \mu
\]
Note that $\mu(B)$ and $\mu(B\stcmp)$ are both in $(0, 1)$ and $\mu_i\neq \mu$ for either $i$.
Hence $\mu$ cannot be extremal.
\end{proof}

For the opposite direction we need the Radon-Nikodym Theorem.

\section{Entropy}
The motivation for a definition of entropy is as a vehicle to distinguish between dynamical systems. First we need to know how tell when two systems are identical.
\begin{defin}
	Two probability spaces with measure preserving transformations, $(X,\mathcal{B},\mu,T),(Y,\mathcal{C}, \nu, S)$ are \mdf{measure-theoretically isomorphic} if there exists a bijection $\pi:B\to C$ where $B\in\mathcal{B}$ and $C\in\mathcal{C}$ such that
	\begin{itemize}
		\item $\mu(B)=\nu(C)=1$
		\item $T(B)\subseteq B, S(C)\subseteq C$
		\item $\pi: B\to C$ and $\pi^{-1}:C\to B$ are measure preserving transformations
		\item $\pi\circ T = S\circ\pi$
		\begin{figure}[H]
			\centering
			\begin{tikzcd}
				X \arrow[r, "T"] \arrow[d, "\pi"'] & X \arrow[d, "\pi"] \\
				Y \arrow[r, "S"'] & Y
			\end{tikzcd}
		\end{figure}	
	\end{itemize}
	
Assume $\msrspc$ is a probability space and $\alpha=\{A_i\}$ a countable collection of subsets $A_i\subseteq B$.
\begin{itemize}
	\item We say $\alpha$ is a \mdf{partition} of $X$ if $\cup A_i$ = X and $A_i\cap A_j=\emptyset$ up to measure 0.
	\item The \mdf{join} of two partitions $\alpha,\beta$ is the partition $\alpha\vee\beta$ of all possible intersections $A_i\cap B_j$.
	\item A countable partition $\beta$ is a \mdf{refinement} of $\alpha$ if every element of $\alpha$ is a union of element of $\beta$ and write $\alpha\leq\beta$.
	\item $\alpha,\beta$ are \mdf{independent} if $\mu(A\cap B)=\mu(A)\mu(B)$ for all $A\in\alpha, B\in\beta$.
	\item The \mdf{information of a partition} $\alpha$ is
		$$I(\alpha)\defeq-\sum_{A\in\alpha}\indic{A}\log(\mu(A))$$
		where $I(\alpha):X\to[0,\infty]$.
	\item The \mdf{entropy of a partition} $\alpha$ is
		$$H(\alpha)\defeq\int_X I(\alpha)d\mu=-\sum_{A\in\alpha}\mu(A)\log(\mu(A))$$
		using the convention $0\cdot\log(0)=0$.
	\item The \mdf{expectation given a partition} is
		$$\expg{\cdot}{\alpha}\defeq\expg{\cdot}{\sigma(\alpha)}$$
	\item The \mdf{conditional probability} of $B\in\mathcal{B}$ given $\alpha$ is
		$$\probg{B}{\alpha}\defeq\expg{\indic{B}}{\alpha}$$
\end{itemize}
Suppose that $\mathcal{C}$ is a sub $\sigma$-algebra of $\mathcal{B}$.
\begin{itemize}
	\item The \mdf{conditional information of $\alpha$ given $\mathcal{C}$} is
		$$\infog{\alpha}{\mathcal{C}}\defeq - \sum_{A\in\alpha}\indic{A}\log(\mu\gvn{A}{C})$$
	where $\mu\gvn{A}{\mathcal{C}}\defeq\expg{\indic{A}}{\mathcal{C}}$
	\item The \mdf{conditional entropy of $\alpha$ given $\mathcal{C}$} is
			$$\entrg{\alpha}{\mathcal{C}}\defeq\int_X\infog{\alpha}{\mathcal{C}}d\mu$$	
\end{itemize}
\end{defin}
We have the following desirable properties:
\begin{itemize}
	\item If $\alpha$ and $\beta$ are independent then $I(\alpha\vee\beta)=I(\alpha)+I(\beta)$.
	\item If $\alpha=\{X\}$ then $I(\alpha)=0$ so $H(\alpha)=0$.
	\item If $T$ is a measure preserving transformation then $H(T^{-1}\alpha)=H(\alpha)$.
	\item Given $A\in\alpha$, $\restr{\expg{f}{\alpha}}{A}=\frac{\int_A f \;d\mu}{\mu(A)}$ and hence
		$$\expg{f}{\alpha}=\sum_{A\in\alpha}\indic{A}\frac{\int_A f \;d\mu}{\mu(A)}$$
	\item Conditional probability and expectation are constant on partition elements.
	\item For $A\in\alpha$, 
		$$\restr{\probg{B}{\alpha}}{A}=\restr{\expg{\indic{B}}{\alpha}}{A}=\frac{\int_A \indic{B} d\mu}{\mu(A)}=\frac{\mu(A\cap B}{\mu(A)}$$
	\item If $\mathcal{C}=\{X,\emptyset\}$ then $\infog{\alpha}{\mathcal{C}}=I(\alpha)$ and $\entrg{\alpha}{\mathcal{C}}=H(\alpha)$.
	\item If $g\geq 0$ is $\sigma(\alpha)$-measurable then $\expg{fg}{\sigma(\alpha)}=g\cdot\expg{f}{\sigma(\alpha)}$.
	\item If $T$ is a measure preserving transformation then $\infog{T^{-1}\alpha}{T^{-1}\mathcal{C}}=\infog{\alpha}{\mathcal{C}}\circ T$.
	\item Integrating this gives $\entrg{T^{-1}\alpha}{T^{-1}\mathcal{C}}=\entrg{\alpha}{\mathcal{C}}$.
	\item $\alpha\leq\beta\implies\infog{\alpha}{\beta}=0$.
\end{itemize}
\begin{prop}
	$$\entrg{\alpha}{\mathcal{C}}=-\int_X \sum_{A\in\alpha}\mu\gvn{A}{\mathcal{C}}\log(\mu\gvn{A}{\mathcal{C}})d\mu$$	
\end{prop}
\begin{lemma}[Basic Identity]
	Given $\alpha,\beta,\gamma$ partitions of $X$ then
	$$\infog{\alpha\vee\beta}{\gamma}=\infog{\alpha}{\gamma}+\infog{\beta}{\alpha\vee\gamma}$$
	$$\entrg{\alpha\vee\beta}{\gamma}=\entrg{\alpha}{\gamma}+\entrg{\beta}{\alpha\vee\gamma}$$
\end{lemma}
\begin{cor}
	$$\beta\leq\gamma\implies\infog{\alpha\vee\beta}{\gamma}=\infog{\alpha}{\gamma}$$
\end{cor}
\begin{cor}[Monotonicity of information of entropy]
	$$\alpha\leq\beta\implies\infog{\alpha}{\gamma}\leq\infog{\beta}{\gamma}$$
\end{cor}
\begin{cor}[Anti-monotonicity of entropy]
	$$\beta\leq\gamma\implies\entrg{\alpha}{\beta}\geq\entrg{\alpha}{\gamma}$$
\end{cor}
\begin{cor}
We have the two following properties as well:
\begin{itemize}
	\item $\entrg{\alpha}{\gamma}\leq H(\alpha)$ (because always $\gamma\leq\{X,\emptyset\}$)
	\item $\entrg{\alpha\vee\beta}{\gamma}\leq\entrg{\alpha}{\gamma}+\entrg{\beta}{\gamma}$
\end{itemize}
\end{cor}
So far this does not encapsulate any dynamics of the system and so we must use these concepts to arrive at a definition of entropy which depends on the transformation. For convenience define the following set:
$$\mathcal{P}\defeq\{\alpha\;\text{countable partitions}\;|H(\alpha) < \infty\}$$
Now choose $\alpha\in\mathcal{P}$. Then we define the following:
$$\mdf{H_n(\alpha)}\defeq H(\alpha^n)\quad\text{where}\quad\alpha^n\defeq\bigvee_{j=0}^{n-1}T^{-j}\alpha$$
This has the convenient property that $H_{n+m}(\alpha)\leq H_n(\alpha)+H_m(\alpha)$, i.e. these $H_n$ form a sub-additive sequence $\R$-valued sequence and hence the limit $\mdf{h(T,\alpha)}\defeq\lim_{n\to\infty}\frac{1}{n}H_n(\alpha)$ exists. We call this the \mdf{entropy of T relative to $\alpha$}.
We can then define the \mdf{entropy of T} by taking the supremum:
$$\mdf{h(T)}\defeq\sup_{\alpha\in\mathcal{P}}h(T,\alpha)$$
Having done all this work, this had better be a measure-theoretic isomorphism invariant.
\end{document}
